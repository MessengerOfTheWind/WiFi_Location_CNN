{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e351dd",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier  \n",
    "Tutorial Link: https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f586c34",
   "metadata": {},
   "source": [
    "### Gini Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba345e1",
   "metadata": {},
   "source": [
    "A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created by the split. A perfect separation results in a Gini score of 0, whereas the worst case split that results in 50/50 classes in each group result in a Gini score of 0.5 (for a 2 class problem).  \n",
    "\n",
    "e.g.  \n",
    "proportion = count(class_value) / count(rows)  \n",
    "Proportions ->  \n",
    "    group_1_class_0 = 2 / 2 = 1  \n",
    "    group_1_class_1 = 0 / 2 = 0  \n",
    "    group_2_class_0 = 0 / 2 = 0  \n",
    "    group_2_class_1 = 2 / 2 = 1  \n",
    "    \n",
    "Gini Index calculation ->  \n",
    "gini_index = sum(proportion * (1.0 - proportion))  \n",
    "gini_index = 1.0 - sum(proportion * proportion)  \n",
    "\n",
    "The Gini index for each group must then be weighted by the size of the group, relative to all of the samples in the parent:  \n",
    "\n",
    "gini_index = (1.0 - sum(proportion * proportion)) * (group_size/total_samples)  \n",
    "\n",
    "Gini(group_1) = (1 - (1*1 + 0*0)) * 2/4  \n",
    "Gini(group_1) = 0.0  \n",
    "\n",
    "Gini(group_2) = (1 - (0*0 + 1*1)) * 2/4  \n",
    "Gini(group_2) = 0.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fa9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "    # count all samples at split point\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    # sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        # avoid divide by zero \n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        # score the group based on the score for each class\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p * p\n",
    "        gini += (1.0 - score) * (size / n_instances)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878b7a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# test Gini values\n",
    "print(gini_index([[[1, 1], [1, 0]], [[1, 1], [1, 0]]], [0, 1]))\n",
    "print(gini_index([[[1, 0], [1, 0]], [[1, 1], [1, 1]]], [0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30840c",
   "metadata": {},
   "source": [
    "### Splitting a Dataset  \n",
    "Splitting a dataset means separating a dataset into two lists of rows given the index of an attribute and a split value for that attribute. Once we have the two groups, we can then use our Gini score above to evaluate the cost of the split.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58ade66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split a dataset based on an attribute and an attribute value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd82a366",
   "metadata": {},
   "source": [
    "### Evaluating All Splits  \n",
    "With the Gini function above and the test split function we now have everything we need to evaluate splits. Given a dataset, we must check every value on each attribute as a candidate split, evaluate the cost of the split and find the best possible split we could make.  \n",
    "\n",
    "Once the best split is found, we can use it as a node in our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c35cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best split for a dataset \n",
    "def get_split(dataset):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini = gini_index(groups, class_values)\n",
    "            #print('X%d < %.3f Gini=%.3f' % ((index+1), row[index], gini))\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index':b_index,'value':b_value,'groups':b_groups}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b0a77c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: [X1 < 6.642]\n"
     ]
    }
   ],
   "source": [
    "dataset = [[2.771244718,1.784783929,0],\n",
    " [1.728571309,1.169761413,0],\n",
    " [3.678319846,2.81281357,0],\n",
    " [3.961043357,2.61995032,0],\n",
    " [2.999208922,2.209014212,0],\n",
    " [7.497545867,3.162953546,1],\n",
    " [9.00220326,3.339047188,1],\n",
    " [7.444542326,0.476683375,1],\n",
    " [10.12493903,3.234550982,1],\n",
    " [6.642287351,3.319983761,1]]\n",
    "split = get_split(dataset)\n",
    "print('Split: [X%d < %.3f]' % ((split['index']+1), split['value']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32898fa8",
   "metadata": {},
   "source": [
    "### Build a Tree  \n",
    "\n",
    "We call the above get_split() function using the entire dataset.  \n",
    "Adding more nodes to our tree is more interesting.  \n",
    "Building a tree may be divided into 3 main parts:  \n",
    "\n",
    "1. Terminal Nodes.  \n",
    "2. Recursive Splitting.  \n",
    "3. Building a Tree.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7001b",
   "metadata": {},
   "source": [
    "#### (1) Terminal Nodes  \n",
    "We need to decide when to stop growing a tree.  \n",
    "\n",
    "We can do that using the depth and the number of rows that the node is responsible for in the training dataset.  \n",
    "\n",
    "**Maximum Tree Depth**. This is the maximum number of nodes from the root node of the tree. Once a maximum depth of the tree is met, we must stop splitting adding new nodes. Deeper trees are more complex and are more likely to overfit the training data.  \n",
    "\n",
    "**Minimum Node Records**. This is the minimum number of training patterns that a given node is responsible for. Once at or below this minimum, we must stop splitting and adding new nodes. Nodes that account for too few training patterns are expected to be too specific and are likely to overfit the training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32c2d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes),key=outcomes.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b1e656",
   "metadata": {},
   "source": [
    "#### (2) Recursive Splitting  \n",
    "We know how and when to create terminal nodes, now we can build our tree.  \n",
    "\n",
    "Building a decision tree involves calling the above developed get_split() function over and over again on the groups created for each node.  \n",
    "\n",
    "New nodes added to an existing node are called child nodes. A node may have zero children (a terminal node), one child (one side makes a prediction directly) or two child nodes. We will refer to the child nodes as left and right in the dictionary representation of a given node.  \n",
    "\n",
    "Once a node is created, we can create child nodes recursively on each group of data from the split by calling the same function again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48b2bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create child splits for a node or make terminal\n",
    "def split(node,max_depth,min_size,depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # Check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left)\n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        split(node['right'],max_depth,min_size,depth+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa85bc79",
   "metadata": {},
   "source": [
    "#### (3) Building a Tree  \n",
    "Building the tree involves creating the root node and calling the split() function that then calls itself recursively to build out the whole tree.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbaf4bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size):\n",
    "    root = get_split(train)\n",
    "    print('root before split:',root)\n",
    "    split(root,max_depth, min_size, 1)\n",
    "    print('root after split:',root)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95523f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a decision tree\n",
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node,dict):\n",
    "        print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n",
    "        print_tree(node['left'], depth+1)\n",
    "        print_tree(node['right'], depth+1)\n",
    "    else:\n",
    "        print('%s[%s]' % ((depth*' ', node)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "672270ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root before split: {'index': 0, 'value': 6.642287351, 'groups': ([[2.771244718, 1.784783929, 0], [1.728571309, 1.169761413, 0], [3.678319846, 2.81281357, 0], [3.961043357, 2.61995032, 0], [2.999208922, 2.209014212, 0]], [[7.497545867, 3.162953546, 1], [9.00220326, 3.339047188, 1], [7.444542326, 0.476683375, 1], [10.12493903, 3.234550982, 1], [6.642287351, 3.319983761, 1]])}\n",
      "root after split: {'index': 0, 'value': 6.642287351, 'left': 0, 'right': 1}\n",
      "[X1 < 6.642]\n",
      " [0]\n",
      " [1]\n"
     ]
    }
   ],
   "source": [
    "tree = build_tree(dataset, 1, 1)\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6469266f",
   "metadata": {},
   "source": [
    "### Making a Prediction  \n",
    "Making predictions with a decision tree involves navigating the tree with the specifically provided row of data.  \n",
    "\n",
    "Again, we can implement this using a recursive function, where the same prediction routine is called again with the left or the right child nodes, depending on how the split affects the provided data.  \n",
    "\n",
    "We must check if a child node is either a terminal value to be returned as the prediction, or if it is a dictionary node containing another level of the tree to be considered.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48d08d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82558a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=0, Got=0\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n",
      "Expected=1, Got=1\n"
     ]
    }
   ],
   "source": [
    "#  predict with a stump\n",
    "stump = {'index': 0, 'right': 1, 'value': 6.642287351, 'left': 0}\n",
    "for row in dataset:\n",
    "    prediction = predict(stump, row)\n",
    "    print('Expected=%d, Got=%d' % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba67cb0",
   "metadata": {},
   "source": [
    "### Banknote Dataset\n",
    "\n",
    "We will evaluate the algorithm using k-fold cross-validation with 5 folds. This means that 1372/5=274.4 or just over 270 records will be used in each fold. We will use the helper functions evaluate_algorithm() to evaluate the algorithm with cross-validation and accuracy_metric() to calculate the accuracy of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56c51ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART on the Bank Note dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    file = open(filename, \"rt\")\n",
    "    lines = reader(file)\n",
    "    dataset = list(lines)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset,column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "def evaluate_algorithm(dataset,algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42415b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification and Regression Tree Algorithm\n",
    "def decision_tree(train, test, max_depth, min_size):\n",
    "    tree = build_tree(train, max_depth, min_size)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(tree, row)\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "303fb763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [96.35036496350365, 97.08029197080292, 97.44525547445255, 98.17518248175182, 97.44525547445255]\n",
      "Mean Accuracy: 97.299%\n"
     ]
    }
   ],
   "source": [
    "# Test CART on Bank Note dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'data_banknote_authentication.csv'\n",
    "dataset = load_csv(filename)\n",
    "# convert string attributes to integers\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "max_depth = 5\n",
    "min_size = 10\n",
    "scores = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af465b18",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2725d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=1300, n_features=2, noise=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c099034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3202a4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.40341559, -0.0576188 ],\n",
       "       [ 1.61222063,  0.89683932],\n",
       "       [-1.68343819, -0.80587007],\n",
       "       [ 1.14282281,  0.75193303],\n",
       "       [-2.42424026,  0.8840454 ],\n",
       "       [-0.79252074, -0.11473644],\n",
       "       [-0.30803428,  0.77966053],\n",
       "       [ 0.93567839,  1.27155509],\n",
       "       [ 1.50075979,  0.85022174],\n",
       "       [-0.25879606,  1.59864717]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e9ad05d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    y[i] = np.array(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a6c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[X[i,0],X[i,1],y[i]] for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28fa5def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.403415585238275, -0.05761879703358539, 99.76438806004]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e49a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [np.concatenate((X[i],[y[i]])) for i in range(len(X))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77a90263",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.40341559e+00, -5.76187970e-02,  9.97643881e+01])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f551dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = list(zip(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25469beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.40341559, -0.0576188 ]), 99.76438806004)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4815ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_y = [[y[i]] for i in range(len(y))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ebf3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = np.concatenate((X,transform_y),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "422d1f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.40341559e+00, -5.76187970e-02,  9.97643881e+01])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61bb0b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][-1] - test[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "05ddf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(groups):\n",
    "    mse = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        if size == 0:\n",
    "            continue\n",
    "        y_hat = sum([row[-1] for row in group]) / size\n",
    "        mse += sum([(row[-1] - y_hat)**2 for row in group]) / size \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5e34fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6ee38da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best split for a dataset \n",
    "def get_split(dataset):\n",
    "    b_index, b_value, b_score, b_groups = m.inf, m.inf, m.inf, None\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            mse = MSE(groups)\n",
    "            #print('X%d < %.3f MSE=%.3f' % ((index+1), row[index], mse))\n",
    "            if mse < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], mse, groups\n",
    "    return {'index':b_index,'value':b_value,'groups':b_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6727b16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: [X1 < -0.308]\n"
     ]
    }
   ],
   "source": [
    "split = get_split(data[:10])\n",
    "print('Split: [X%d < %.3f]' % ((split['index']+1), split['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2f365eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.403415585238275, -0.05761879703358539, 99.76438806004], [1.6122206282554243, 0.8968393158655319, 159.639496052825], [-1.6834381922209503, -0.8058700664961888, -153.4926722547169], [1.1428228145150205, 0.7519330326867741, 125.07288207637336], [-2.4242402602729416, 0.8840453963610497, -15.738088311279796], [-0.7925207384327007, -0.11473644146689901, -43.840731443647435], [-0.30803428418574125, 0.7796605322693398, 65.07288857599129], [0.9356783931474612, 1.2715550949941588, 166.96381939017905], [1.5007597906343109, 0.8502217421134929, 149.56874045073454], [-0.25879606266710237, 1.598647170504717, 147.08460389363734]]\n"
     ]
    }
   ],
   "source": [
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "42ccb680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.6834381922209503, -0.8058700664961888, -153.4926722547169], [-2.4242402602729416, 0.8840453963610497, -15.738088311279796], [-0.7925207384327007, -0.11473644146689901, -43.840731443647435]]\n",
      "[[2.403415585238275, -0.05761879703358539, 99.76438806004], [1.6122206282554243, 0.8968393158655319, 159.639496052825], [1.1428228145150205, 0.7519330326867741, 125.07288207637336], [-0.30803428418574125, 0.7796605322693398, 65.07288857599129], [0.9356783931474612, 1.2715550949941588, 166.96381939017905], [1.5007597906343109, 0.8502217421134929, 149.56874045073454], [-0.25879606266710237, 1.598647170504717, 147.08460389363734]]\n"
     ]
    }
   ],
   "source": [
    "bin_1, bin_2 = [], []\n",
    "for x in data[:10]:\n",
    "    if x[-1] < -0.308:\n",
    "        bin_1.append(x)\n",
    "    else:\n",
    "        bin_2.append(x)\n",
    "print(bin_1)\n",
    "print(bin_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4474e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return sum(outcomes)/len(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "40ee14a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create child splits for a node or make terminal\n",
    "def split(node,max_depth,min_size,depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # Check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left)\n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        split(node['right'],max_depth,min_size,depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5b282155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification and Regression Tree Algorithm\n",
    "def decision_tree(train, test, max_depth, min_size):\n",
    "    tree = build_tree(train, max_depth, min_size)\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict(tree, row)\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4c2c32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE\n",
    "def accuracy_metric(actual, predicted):\n",
    "    total = 0\n",
    "    for i in range(len(actual)):\n",
    "        total += (actual[i] - predicted[i])**2 \n",
    "    return total / float(len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c6495001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [4635.889088230275, 5511.951224805913, 5386.04266248655, 5591.2031250409445, 5572.4239600926]\n",
      "AVG MSE: 5339.502\n"
     ]
    }
   ],
   "source": [
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "max_depth = 5\n",
    "min_size = 10\n",
    "scores = evaluate_algorithm(data, decision_tree, n_folds, max_depth, min_size)\n",
    "print('Scores: %s' % scores)\n",
    "print('AVG MSE: %.3f' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ceb65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
