{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:35.453055Z",
     "start_time": "2025-08-21T07:47:35.440054Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 添加验证集和测试集\n",
    "train_df = pd.read_csv('../../UJIIndoorLoc/trainingData.csv')\n",
    "train_df_noisy = pd.read_csv('../../data/train_noisy.csv')\n",
    "train_df_noisy1 = pd.read_csv('../../data/train_noisy1.csv')\n",
    "train_df_noisy2 = pd.read_csv('../../data/train_noisy2.csv')\n",
    "# train_df_noisy3 = pd.read_csv('../data/train_noisy3.csv')\n",
    "train_df_noisy4 = pd.read_csv('../../data/train_noisy4.csv')\n",
    "train_df_noisy5 = pd.read_csv('../../data/train_noisy5.csv')\n",
    "\n",
    "# train_df = pd.concat([train_df, train_df_noisy4], ignore_index=True)\n",
    "train_df = pd.concat([train_df, train_df_noisy1], ignore_index=True)\n",
    "# train_df = pd.concat([train_df, train_df_noisy2], ignore_index=True)\n",
    "# train_df = pd.concat([train_df, train_df_noisy5], ignore_index=True)\n",
    "valid_df = pd.read_csv('../../UJIIndoorLoc/validationData.csv')\n",
    "\n",
    "total_df = pd.concat([train_df, valid_df])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:38.449761Z",
     "start_time": "2025-08-21T07:47:35.454055Z"
    }
   },
   "id": "5b43f0c48310a7d7",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -7541.2643 4864920.7782]\n"
     ]
    }
   ],
   "source": [
    "# 训练集特征\n",
    "training_data = train_df[train_df.columns[:520]].to_numpy()\n",
    "# training_data[training_data == 100] = -110\n",
    "training_floors = train_df['FLOOR'].to_numpy() # FLOOR LABELS\n",
    "training_buildings = train_df['BUILDINGID'].to_numpy() # BUILDING LABELS\n",
    "training_longitude = train_df['LONGITUDE'].to_numpy() # LONGITUDE LABELS\n",
    "training_latitude = train_df['LATITUDE'].to_numpy() # LATITUDE LABELS\n",
    "training_coords = train_df[['LONGITUDE','LATITUDE']].to_numpy() # LONGITUDE + LATITUDE LABELS\n",
    "print(training_coords[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:38.574746Z",
     "start_time": "2025-08-21T07:47:38.450754Z"
    }
   },
   "id": "2c440946795b99bb",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 验证机特征\n",
    "valid_data = valid_df[valid_df.columns[:520]].to_numpy()\n",
    "# valid_data[valid_data == 100] = -110\n",
    "valid_floors = valid_df['FLOOR'].to_numpy() # FLOOR LABELS\n",
    "valid_buildings = valid_df['BUILDINGID'].to_numpy() # BUILDING LABELS\n",
    "valid_longitude = valid_df['LONGITUDE'].to_numpy() # LONGITUDE LABELS\n",
    "valid_latitude = valid_df['LATITUDE'].to_numpy() # LATITUDE LABELS\n",
    "valid_coords = valid_df[['LONGITUDE','LATITUDE']].to_numpy() # LONGITUDE + LATITUDE LABELS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:38.590206Z",
     "start_time": "2025-08-21T07:47:38.575748Z"
    }
   },
   "id": "5016b09f12bb9bb8",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "totaling_data = total_df[total_df.columns[:520]].to_numpy()\n",
    "# totaling_data[totaling_data == 100] = -110\n",
    "totaling_floors = total_df['FLOOR'].to_numpy() # FLOOR LABELS\n",
    "totaling_buildings = total_df['BUILDINGID'].to_numpy() # BUILDING LABELS\n",
    "totaling_longitude = total_df['LONGITUDE'].to_numpy() # LONGITUDE LABELS\n",
    "totaling_latitude = total_df['LATITUDE'].to_numpy() # LATITUDE LABELS\n",
    "totaling_coords = total_df[['LONGITUDE','LATITUDE']].to_numpy() # LONGITUDE + LATITUDE LABELS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:38.717606Z",
     "start_time": "2025-08-21T07:47:38.591207Z"
    }
   },
   "id": "106ac514e54b99b4",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from Algorithms.utill.ELM import ELM_AE\n",
    "# # 2. 标准化\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(training_data)\n",
    "# \n",
    "# # 3. ELM降维\n",
    "# elm_ae = ELM_AE(input_dim=training_data.shape[1], hidden_dim=256)\n",
    "# training_data = elm_ae.fit_transform(X_scaled)\n",
    "# \n",
    "# print(training_data[0])\n",
    "# print(totaling_coords[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:38.733606Z",
     "start_time": "2025-08-21T07:47:38.718607Z"
    }
   },
   "id": "9090dc53583f3d2e",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-105.41166034560202 100.0\n",
      "[  -7541.2643 4864920.7782]\n",
      "[  -7541.2643 4864920.7782]\n",
      "4864745.745015971 4865017.364684202 -7695.93875492993 -7299.786516730871\n"
     ]
    }
   ],
   "source": [
    "from Algorithms.utill.data_standar import normalize_rssi, normalize_coords, normalize_test_or_valid_data\n",
    "# 数据标准化,从总数居获取最大值最小值\n",
    "X_totalCo_cnn, X_min, X_max = normalize_rssi(totaling_data)\n",
    "print(X_min, X_max)\n",
    "# 训练集特征标准化\n",
    "training_data = normalize_test_or_valid_data(X_min, X_max, training_data)\n",
    "# 验证集特征标准化\n",
    "valid_data = normalize_test_or_valid_data(X_min, X_max, valid_data)\n",
    "# 训练集经度标签标准化\n",
    "print(training_coords[0])\n",
    "training_latitude, y_min_la, y_max_la = normalize_coords(totaling_latitude, training_latitude)\n",
    "# 验证集经度标签标准化\n",
    "valid_latitude, y_va_min_la, y_va_max_la = normalize_coords(totaling_latitude, valid_latitude)\n",
    "# 训练集纬度标签标准化\n",
    "print(training_coords[0])\n",
    "training_longitude, y_min_lo, y_max_lo = normalize_coords(totaling_longitude, training_longitude)\n",
    "# 验证集纬度标签标准化\n",
    "valid_longitude, y_va_min, y_va_max = normalize_coords(totaling_longitude, valid_longitude)\n",
    "# 保存的最大，最小值\n",
    "print(y_min_la, y_max_la, y_min_lo, y_max_lo)\n",
    "min_max_dist = [y_min_la, y_max_la, y_min_lo, y_max_lo]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:38.906023Z",
     "start_time": "2025-08-21T07:47:38.734297Z"
    }
   },
   "id": "ed79bf7b866c21aa",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39874, 520)\n"
     ]
    }
   ],
   "source": [
    "print(training_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:38.922030Z",
     "start_time": "2025-08-21T07:47:38.907023Z"
    }
   },
   "id": "13c5e8d81eb4dafd",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Augmenting data shape to fit 23x23\n",
    "training_data_aug = np.empty((len(training_data),529))\n",
    "for x in range(len(training_data)):\n",
    "    training_data_aug[x] = np.concatenate((training_data[x], np.full(shape=9,fill_value=1)))\n",
    "# Augmenting data shape to fit 23*23\n",
    "valid_data_aug = np.empty((len(valid_data),529))\n",
    "for x in range(len(valid_data)):\n",
    "    valid_data_aug[x] = np.concatenate((valid_data[x], np.full(shape=9,fill_value=1)))\n",
    "# valid_data_aug.shape\n",
    "# training_data_aug = training_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:39.076669Z",
     "start_time": "2025-08-21T07:47:38.923026Z"
    }
   },
   "id": "8e84b859f7ab2b1",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 经度处理\n",
    "from sklearn.model_selection import train_test_split\n",
    "#  准备数据\n",
    "X = training_data_aug.reshape(len(training_data_aug), 23, 23)\n",
    "y = training_latitude\n",
    "z = train_df['SPACEID'].to_numpy()  # 分层标签\n",
    "\n",
    "assert len(X) == len(y) == len(z), (len(X), len(y), len(z))\n",
    "\n",
    "# 第一步：先切 20% 测试集\n",
    "X_temp_la, X_test_la_cnn, y_temp_la, y_test_la_cnn, stratify_temp_la, stratify_test_la = train_test_split(\n",
    "    X, y, z,\n",
    "    test_size=0.2,               # 20% → 测试\n",
    "    random_state=2812,\n",
    "    stratify=z\n",
    ")\n",
    "\n",
    "# 第二步：在剩下的 80% 里切 10% 验证 (0.8 * 1/8 = 0.1)\n",
    "X_train_la_cnn, X_valid_la_cnn, y_train_la_cnn, y_valid_la_cnn = train_test_split(\n",
    "    X_temp_la, y_temp_la,\n",
    "    test_size=1/8,               # 10% → 验证\n",
    "    random_state=2812,\n",
    "    stratify=stratify_temp_la\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:39.170463Z",
     "start_time": "2025-08-21T07:47:39.078662Z"
    }
   },
   "id": "e5b959da04e7e45d",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 纬度处理\n",
    "from sklearn.model_selection import train_test_split\n",
    "#  准备数据\n",
    "X = training_data_aug.reshape(len(training_data_aug), 23, 23)\n",
    "y = training_longitude\n",
    "z = train_df['SPACEID'].to_numpy()  # 分层标签\n",
    "\n",
    "assert len(X) == len(y) == len(z), (len(X), len(y), len(z))\n",
    "\n",
    "# 第一步：先切 20% 测试集\n",
    "X_temp_lo, X_test_lo_cnn, y_temp_lo, y_test_lo_cnn, stratify_temp_lo, stratify_test_lo = train_test_split(\n",
    "    X, y, z,\n",
    "    test_size=0.2,               # 20% → 测试\n",
    "    random_state=2812,\n",
    "    stratify=z\n",
    ")\n",
    "\n",
    "# 第二步：在剩下的 80% 里切 10% 验证 (0.8 * 1/8 = 0.1)\n",
    "X_train_lo_cnn, X_valid_lo_cnn, y_train_lo_cnn, y_valid_lo_cnn = train_test_split(\n",
    "    X_temp_lo, y_temp_lo,\n",
    "    test_size=1/8,               # 10% → 验证\n",
    "    random_state=2812,\n",
    "    stratify=stratify_temp_lo\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:39.265370Z",
     "start_time": "2025-08-21T07:47:39.170463Z"
    }
   },
   "id": "64cbe552698df8ad",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# \n",
    "# train_space_ids = train_df['SPACEID'].values\n",
    "# X_train_la_cnn, X_test_la_cnn, y_train_la_cnn, y_test_la_cnn = train_test_split(training_data_aug.reshape(len(training_data_aug),23,23), training_latitude, test_size=0.2, random_state=2812,stratify=train_space_ids)\n",
    "# # X_trainCo_cnn, y_trainCo_cnn = training_data_aug.reshape(len(training_data_aug),23,23), training_coords\n",
    "# X_valid_la_cnn, y_valid_la_cnn = valid_data_aug.reshape(len(valid_data_aug),23,23), valid_latitude\n",
    "# \n",
    "# \n",
    "# valid_ids =  valid_df['SPACEID'].values\n",
    "# # 只取纬度作为训练\n",
    "# X_train_lo_cnn, X_test_lo_cnn, y_train_lo_cnn, y_test_lo_cnn = train_test_split(training_data_aug.reshape(len(training_data_aug),23,23), training_longitude, test_size=0.2, random_state=2812, stratify=train_space_ids)\n",
    "# # X_trainCo_cnn, y_trainCo_cnn = training_data_aug.reshape(len(training_data_aug),23,23), training_coords\n",
    "# X_valid_lo_cnn, y_valid_lo_cnn = valid_data_aug.reshape(len(valid_data_aug),23,23), valid_longitude"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:39.281088Z",
     "start_time": "2025-08-21T07:47:39.266669Z"
    }
   },
   "id": "b561f91729ebaaf6",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22083115 0.64440394 0.29282373 0.25023918 0.22978448]\n",
      "[0.05350294 0.14914304 0.50588378 0.25023918 0.44562341]\n",
      "[0.96960011 0.39044398 0.90425503 0.95688354 0.73482623]\n",
      "[0.83985521 0.75911613 0.66512474 0.95688354 0.33762918]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_la_cnn[:5])\n",
    "print(y_test_la_cnn[:5])\n",
    "print(y_train_lo_cnn[:5])\n",
    "print(y_test_lo_cnn[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:39.297044Z",
     "start_time": "2025-08-21T07:47:39.282082Z"
    }
   },
   "id": "6aee87825d86445",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "single_image = X_train_la_cnn[0]\n",
    "plt.imshow(single_image)\n",
    "plt.title(f'Label: {X_valid_la_cnn[0]}')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:39.515177Z",
     "start_time": "2025-08-21T07:47:39.298189Z"
    }
   },
   "id": "44fbae7440de8a89",
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from Algorithms.CNN import CNNRegressor\n",
    "from Algorithms.utill.data_standar import denormalize_coords, mean_euclidean_distance"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T07:47:39.531172Z",
     "start_time": "2025-08-21T07:47:39.516170Z"
    }
   },
   "id": "278f630db29249e0",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Dropout: 0.3) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0336\n",
      "70.12963676525955\n",
      "103.28940144389891\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0041\n",
      "17.353933969023547\n",
      "25.608027847510474\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0041\n",
      "14.836502066249631\n",
      "21.921667634811993\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0033\n",
      "13.565677918943543\n",
      "20.015725858118145\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0029\n",
      "13.134693343124702\n",
      "19.344948504032427\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0032\n",
      "12.19759350036403\n",
      "18.01330748205397\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0035\n",
      "11.507793987145202\n",
      "17.02763172260501\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0035\n",
      "11.027292436545254\n",
      "16.335595040608002\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0037\n",
      "10.552722822476643\n",
      "15.647587369180425\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0037\n",
      "10.077555663898067\n",
      "14.95953286151823\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0037\n",
      "9.857435964477917\n",
      "14.637348414684206\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0036\n",
      "9.73900990928872\n",
      "14.472499489695815\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0035\n",
      "9.60758383791757\n",
      "14.285305082644076\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0037\n",
      "9.496756492570677\n",
      "14.124078987471473\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0035\n",
      "9.485782313485998\n",
      "14.113621131476\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0032\n",
      "9.397867801754831\n",
      "13.979474073412485\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0033\n",
      "9.416534405262269\n",
      "14.013512226195942\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0033\n",
      "9.37625865098719\n",
      "13.950402545061197\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0032\n",
      "9.328592128643066\n",
      "13.872126544326655\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0031\n",
      "9.256568993544132\n",
      "13.761008566319763\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0031\n",
      "9.239492909822236\n",
      "13.73403762307803\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0031\n",
      "9.181562221432507\n",
      "13.651657594115676\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0030\n",
      "9.18081434827744\n",
      "13.643671046352521\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0030\n",
      "9.149884131139473\n",
      "13.594753965645328\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0029\n",
      "9.195953304549128\n",
      "13.658973986548675\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0028\n",
      "9.254488456254203\n",
      "13.737797432489195\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0029\n",
      "9.110764907168484\n",
      "13.527232399996631\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0027\n",
      "9.33911851808266\n",
      "13.858008608109987\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [29/100], Step [437/437], Loss: 0.0027\n",
      "9.344473941978315\n",
      "13.864703834146257\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [30/100], Step [437/437], Loss: 0.0027\n",
      "9.376902511525246\n",
      "13.909006290449359\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [31/100], Step [437/437], Loss: 0.0027\n",
      "9.3798994568179\n",
      "13.91726795156663\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:30 batch_size64 lr:0.0001 kernel_size3 stride:1 train_acc:9.110764907168484 test_acc:13.527232399996631\n",
      "Best model parameters loaded: ./model/82lo.pth\n",
      "Best model parameters loaded../model/82lo.pth\n",
      "(Dropout: 0.3) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0510\n",
      "76.16805979200176\n",
      "111.94000044013875\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0092\n",
      "16.53871836776748\n",
      "24.095307105244196\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0067\n",
      "14.45857642265342\n",
      "21.017440085521198\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0047\n",
      "13.793337362624655\n",
      "20.091392825084235\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0035\n",
      "13.078584291970317\n",
      "19.12442995630139\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0032\n",
      "12.014165190157629\n",
      "17.65984113314803\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0031\n",
      "10.747885954024978\n",
      "15.906041271411544\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0033\n",
      "9.904815368927075\n",
      "14.72271393337291\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0031\n",
      "9.423488555554115\n",
      "14.052494320178194\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0028\n",
      "8.876570708421758\n",
      "13.299482016409769\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0025\n",
      "8.706564381503448\n",
      "13.039215863210133\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0023\n",
      "8.536960768669976\n",
      "12.810950737378288\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0021\n",
      "8.276976339056182\n",
      "12.434253472870035\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0019\n",
      "8.255556122690031\n",
      "12.404812274567737\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0016\n",
      "8.261947067962712\n",
      "12.422562669592562\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0015\n",
      "8.163904907445362\n",
      "12.29632180985058\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0014\n",
      "8.02919392293971\n",
      "12.114789418481147\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0014\n",
      "7.861170782668925\n",
      "11.882901675781005\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0013\n",
      "7.793470060974921\n",
      "11.800031044205033\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0012\n",
      "7.519937772795758\n",
      "11.422375270332587\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0011\n",
      "7.50365754762079\n",
      "11.41611675671451\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0011\n",
      "7.357752331053813\n",
      "11.217550959201787\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0010\n",
      "7.44472669194298\n",
      "11.338363834265648\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0010\n",
      "7.306560377566034\n",
      "11.154720409897154\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0009\n",
      "7.098721384585909\n",
      "10.863353672220207\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0009\n",
      "7.021995054892363\n",
      "10.772541437986531\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0010\n",
      "7.141186208012308\n",
      "10.941658424381991\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0009\n",
      "6.993457074259818\n",
      "10.74189923434083\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/100], Step [437/437], Loss: 0.0009\n",
      "6.981046733502277\n",
      "10.726164280681424\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/100], Step [437/437], Loss: 0.0008\n",
      "6.9760088874700825\n",
      "10.72958629226966\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [31/100], Step [437/437], Loss: 0.0008\n",
      "6.989188106957079\n",
      "10.749071589886714\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [32/100], Step [437/437], Loss: 0.0008\n",
      "6.847389239599634\n",
      "10.566103064631934\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/100], Step [437/437], Loss: 0.0008\n",
      "6.869485889540419\n",
      "10.596210949112438\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [34/100], Step [437/437], Loss: 0.0008\n",
      "6.703553633266876\n",
      "10.374167870693336\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/100], Step [437/437], Loss: 0.0008\n",
      "6.841889691551422\n",
      "10.564643529653972\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [36/100], Step [437/437], Loss: 0.0009\n",
      "6.893151756124603\n",
      "10.630872933952928\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [37/100], Step [437/437], Loss: 0.0008\n",
      "6.667664620412261\n",
      "10.324846983807523\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [38/100], Step [437/437], Loss: 0.0008\n",
      "6.778022979865643\n",
      "10.47780049971236\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [39/100], Step [437/437], Loss: 0.0008\n",
      "6.8481218706844995\n",
      "10.562110350668565\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [40/100], Step [437/437], Loss: 0.0007\n",
      "6.677152743252971\n",
      "10.341881314049472\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [41/100], Step [437/437], Loss: 0.0007\n",
      "6.5373309033874705\n",
      "10.149079680720149\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [42/100], Step [437/437], Loss: 0.0008\n",
      "6.7898132731039045\n",
      "10.490722679142161\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [43/100], Step [437/437], Loss: 0.0007\n",
      "6.615635286157849\n",
      "10.257929217699962\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [44/100], Step [437/437], Loss: 0.0007\n",
      "6.600785131338326\n",
      "10.233938534040615\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [45/100], Step [437/437], Loss: 0.0007\n",
      "6.569551515465834\n",
      "10.182581033821648\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:44 batch_size64 lr:0.0001 kernel_size3 stride:1 train_acc:6.5373309033874705 test_acc:10.149079680720149\n",
      "Best model parameters loaded: ./model/83lo.pth\n",
      "Best model parameters loaded../model/83lo.pth\n",
      "(Dropout: 0.3) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0603\n",
      "54.53975069183813\n",
      "80.63098273957192\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0161\n",
      "16.942895916053672\n",
      "25.07490757261365\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0122\n",
      "14.203273460235039\n",
      "20.967595115036758\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0089\n",
      "14.108060024249033\n",
      "20.80910807385141\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0073\n",
      "12.156302348477258\n",
      "18.02106031744772\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0069\n",
      "11.608299952227279\n",
      "17.244280164556862\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0057\n",
      "11.695261375902911\n",
      "17.372059778819896\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0054\n",
      "10.449660041165021\n",
      "15.63921923251347\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0053\n",
      "9.795436545450832\n",
      "14.726889990222075\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0051\n",
      "9.360363154087754\n",
      "14.117551507177945\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0049\n",
      "9.185932444870478\n",
      "13.866170766146533\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0046\n",
      "9.002376324402466\n",
      "13.602840330405183\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0045\n",
      "8.791587996516176\n",
      "13.297552810682156\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0043\n",
      "8.629572517708139\n",
      "13.058520802143947\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0042\n",
      "8.561786640381401\n",
      "12.969925627872156\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0041\n",
      "8.38126423214577\n",
      "12.706470066581574\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0040\n",
      "8.276720333229525\n",
      "12.563126073439136\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0040\n",
      "8.21203964430419\n",
      "12.466920423543087\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0039\n",
      "8.13319759511946\n",
      "12.354844643159545\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0038\n",
      "8.055184839061122\n",
      "12.240973977645295\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0038\n",
      "7.991399593483456\n",
      "12.148492950305485\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0037\n",
      "7.920715582169425\n",
      "12.045517115838596\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0036\n",
      "7.860146253884643\n",
      "11.951961506513848\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0036\n",
      "7.80634754896848\n",
      "11.877269330211845\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0036\n",
      "7.757009971101066\n",
      "11.802320521973739\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0035\n",
      "7.736190180667065\n",
      "11.767399695317588\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0035\n",
      "7.742592632796131\n",
      "11.774661664700655\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0034\n",
      "7.599372998577264\n",
      "11.56595943898352\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/100], Step [437/437], Loss: 0.0033\n",
      "7.580207465940093\n",
      "11.545075947571766\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/100], Step [437/437], Loss: 0.0033\n",
      "7.589053343128629\n",
      "11.551670726558791\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [31/100], Step [437/437], Loss: 0.0031\n",
      "7.555302108803826\n",
      "11.498225124958873\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/100], Step [437/437], Loss: 0.0031\n",
      "7.528671382804446\n",
      "11.450636431101612\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/100], Step [437/437], Loss: 0.0030\n",
      "7.496734539401515\n",
      "11.403876770277906\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/100], Step [437/437], Loss: 0.0029\n",
      "7.503398055260093\n",
      "11.412420020841967\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [35/100], Step [437/437], Loss: 0.0029\n",
      "7.479966358082734\n",
      "11.376255592959973\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [36/100], Step [437/437], Loss: 0.0027\n",
      "7.472288918652046\n",
      "11.361122663461963\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [37/100], Step [437/437], Loss: 0.0027\n",
      "7.458480736274913\n",
      "11.33369043700175\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [38/100], Step [437/437], Loss: 0.0026\n",
      "7.455593056781595\n",
      "11.323202668522521\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [39/100], Step [437/437], Loss: 0.0026\n",
      "7.434929331196529\n",
      "11.297727717165259\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [40/100], Step [437/437], Loss: 0.0025\n",
      "7.42378934069472\n",
      "11.280571693006939\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [41/100], Step [437/437], Loss: 0.0025\n",
      "7.433097004732146\n",
      "11.293274772137398\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [42/100], Step [437/437], Loss: 0.0024\n",
      "7.398243917162196\n",
      "11.244695126460547\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [43/100], Step [437/437], Loss: 0.0024\n",
      "7.423102074198593\n",
      "11.275170980966895\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [44/100], Step [437/437], Loss: 0.0023\n",
      "7.388943771510461\n",
      "11.221818395737737\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [45/100], Step [437/437], Loss: 0.0023\n",
      "7.392862033419198\n",
      "11.224626572466624\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [46/100], Step [437/437], Loss: 0.0023\n",
      "7.379444959729407\n",
      "11.207294781643645\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [47/100], Step [437/437], Loss: 0.0023\n",
      "7.404876903247733\n",
      "11.231617084364034\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [48/100], Step [437/437], Loss: 0.0023\n",
      "7.3917664262533505\n",
      "11.212368197831461\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [49/100], Step [437/437], Loss: 0.0023\n",
      "7.3838768898449745\n",
      "11.204914165309637\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [50/100], Step [437/437], Loss: 0.0024\n",
      "7.396657182918362\n",
      "11.223061416874815\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [51/100], Step [437/437], Loss: 0.0022\n",
      "7.40338825254782\n",
      "11.228823500853133\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [52/100], Step [437/437], Loss: 0.0022\n",
      "7.378571167640588\n",
      "11.19742305082514\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [53/100], Step [437/437], Loss: 0.0021\n",
      "7.398528576284147\n",
      "11.218018087673897\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [54/100], Step [437/437], Loss: 0.0021\n",
      "7.416007828332948\n",
      "11.233893700437408\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [55/100], Step [437/437], Loss: 0.0021\n",
      "7.49773344555824\n",
      "11.342601821454343\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [56/100], Step [437/437], Loss: 0.0020\n",
      "7.583885090191604\n",
      "11.468345506726132\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:55 batch_size64 lr:0.0001 kernel_size5 stride:1 train_acc:7.378571167640588 test_acc:11.19742305082514\n",
      "Best model parameters loaded: ./model/84lo.pth\n",
      "Best model parameters loaded../model/84lo.pth\n",
      "(Dropout: 0.3) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0427\n",
      "55.379877195166145\n",
      "81.874527600948\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0119\n",
      "15.47973415487428\n",
      "22.902808784633677\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0072\n",
      "13.563684256677893\n",
      "20.034058397964486\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0062\n",
      "11.242246985652843\n",
      "16.743700579009946\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0051\n",
      "11.166485182554396\n",
      "16.6077442324601\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0041\n",
      "10.074083106757568\n",
      "15.104741840851657\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0035\n",
      "9.167975520716318\n",
      "13.883239035050986\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0029\n",
      "8.821159264289765\n",
      "13.432144756950382\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0024\n",
      "8.142954744770776\n",
      "12.535051211561292\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0021\n",
      "7.55613514083192\n",
      "11.733915533967131\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0017\n",
      "7.200081556508672\n",
      "11.261016827160972\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0016\n",
      "6.872770168759039\n",
      "10.820355648371788\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0014\n",
      "6.579656612031821\n",
      "10.41077449294571\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0012\n",
      "6.464973028841889\n",
      "10.252713315445327\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0011\n",
      "6.156904766763689\n",
      "9.842590160495444\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0010\n",
      "6.0491787286943515\n",
      "9.671206598988896\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0008\n",
      "6.038014614077591\n",
      "9.655876905037795\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0007\n",
      "5.907871913537069\n",
      "9.475663741509312\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0007\n",
      "5.898855539787513\n",
      "9.447758913553514\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0006\n",
      "5.890694485591029\n",
      "9.428411021195505\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0005\n",
      "5.8716083439942235\n",
      "9.399496329157\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0005\n",
      "6.041033881133699\n",
      "9.628705262463802\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0005\n",
      "6.030628813098593\n",
      "9.601005396306052\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0004\n",
      "6.122871946029161\n",
      "9.715866208806618\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0004\n",
      "6.138421281612062\n",
      "9.74015728777154\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:24 batch_size64 lr:0.0001 kernel_size5 stride:1 train_acc:5.8716083439942235 test_acc:9.399496329157\n",
      "Best model parameters loaded: ./model/85lo.pth\n",
      "Best model parameters loaded../model/85lo.pth\n",
      "(Dropout: 0.3) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0459\n",
      "49.653134862001515\n",
      "73.27744138858131\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0147\n",
      "20.510169083961376\n",
      "30.71724821480269\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0078\n",
      "16.18085938904067\n",
      "24.26789029170589\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0063\n",
      "13.833153802037886\n",
      "20.7822562340427\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0062\n",
      "12.239588451917484\n",
      "18.418504801708256\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0060\n",
      "11.64735616880068\n",
      "17.49861978402586\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0053\n",
      "11.408141130279455\n",
      "17.081059763045747\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0048\n",
      "10.887430779519454\n",
      "16.30641019291794\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0044\n",
      "10.403449437791918\n",
      "15.592081013043096\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0042\n",
      "9.76865453898243\n",
      "14.687258754103594\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0040\n",
      "9.245756874883906\n",
      "13.938773627757287\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0037\n",
      "8.826602631896598\n",
      "13.343275802594125\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0035\n",
      "8.5241093593271\n",
      "12.910695767442066\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0033\n",
      "8.20787987241988\n",
      "12.463274610850052\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0031\n",
      "8.190074714364261\n",
      "12.406478189852542\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0029\n",
      "8.014197324632066\n",
      "12.151589383824088\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0027\n",
      "8.266991890213019\n",
      "12.490966813766429\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0026\n",
      "8.086513505620916\n",
      "12.234549661158562\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0025\n",
      "8.167331326955775\n",
      "12.350039546916934\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0024\n",
      "7.816558212061942\n",
      "11.86273183710428\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0024\n",
      "7.671792763299487\n",
      "11.65060273078206\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0022\n",
      "7.699711863963322\n",
      "11.692546373048996\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0022\n",
      "7.700701150503223\n",
      "11.693518007991328\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0022\n",
      "7.728169347845001\n",
      "11.721886942489403\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0021\n",
      "7.677019610991938\n",
      "11.65139839476338\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:24 batch_size64 lr:0.0001 kernel_size7 stride:1 train_acc:7.671792763299487 test_acc:11.65060273078206\n",
      "Best model parameters loaded: ./model/86lo.pth\n",
      "Best model parameters loaded../model/86lo.pth\n",
      "(Dropout: 0.3) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0302\n",
      "34.56376226208688\n",
      "51.06914570107694\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0089\n",
      "15.025218401062402\n",
      "22.35899029954295\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0053\n",
      "12.180940818558227\n",
      "18.15184191073437\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0043\n",
      "10.942774360834129\n",
      "16.323944946062422\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0033\n",
      "10.1204083685034\n",
      "15.136363537956191\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0027\n",
      "9.492710801263717\n",
      "14.247460335011352\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0019\n",
      "9.032675229196322\n",
      "13.589248008552532\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0012\n",
      "8.68712452680143\n",
      "13.100973892784944\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0009\n",
      "8.039709840377625\n",
      "12.205557357601164\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0006\n",
      "7.365087916762874\n",
      "11.272694271881024\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0004\n",
      "7.175313418713075\n",
      "11.000108354749683\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0003\n",
      "6.872918603091859\n",
      "10.571903518296915\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0002\n",
      "6.78784080209903\n",
      "10.451415441543853\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0001\n",
      "6.350088636110377\n",
      "9.846291364610536\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0001\n",
      "6.261258860508187\n",
      "9.7365438104356\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0001\n",
      "5.930245774835734\n",
      "9.28063985210039\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0001\n",
      "5.645880489090235\n",
      "8.902585647735942\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0001\n",
      "5.422504251085472\n",
      "8.625247152126331\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0001\n",
      "5.153897850373551\n",
      "8.280673732162073\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0001\n",
      "5.022586506894386\n",
      "8.10554110466039\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0001\n",
      "4.86741603861906\n",
      "7.919979587758571\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0001\n",
      "4.792826937877672\n",
      "7.827776883945882\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0001\n",
      "4.81659578806936\n",
      "7.875326214438643\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0001\n",
      "4.797000673665543\n",
      "7.88434048463568\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0001\n",
      "4.728131351633053\n",
      "7.782378450888673\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0001\n",
      "4.537063837523209\n",
      "7.480407968245762\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0000\n",
      "4.848766346887941\n",
      "7.85682123468284\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0000\n",
      "4.602723826357685\n",
      "7.543138102931352\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [29/100], Step [437/437], Loss: 0.0000\n",
      "4.398764750539579\n",
      "7.304100830639294\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/100], Step [437/437], Loss: 0.0000\n",
      "4.338961886386681\n",
      "7.247898727722472\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/100], Step [437/437], Loss: 0.0000\n",
      "4.29695250267128\n",
      "7.212065343300227\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/100], Step [437/437], Loss: 0.0000\n",
      "4.089497484255046\n",
      "6.933669096005757\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/100], Step [437/437], Loss: 0.0000\n",
      "4.354459636886437\n",
      "7.291322890958797\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [34/100], Step [437/437], Loss: 0.0000\n",
      "4.438853271990397\n",
      "7.400367953428681\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [35/100], Step [437/437], Loss: 0.0000\n",
      "4.513622042867482\n",
      "7.4992991709074035\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [36/100], Step [437/437], Loss: 0.0000\n",
      "4.459539735623959\n",
      "7.4287062743771\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:35 batch_size64 lr:0.0001 kernel_size7 stride:1 train_acc:4.089497484255046 test_acc:6.933669096005757\n",
      "Best model parameters loaded: ./model/87lo.pth\n",
      "Best model parameters loaded../model/87lo.pth\n",
      "(Dropout: 0.4) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0366\n",
      "98.01515320718654\n",
      "144.25730591840977\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0109\n",
      "19.873376718089002\n",
      "29.144123610475525\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0080\n",
      "15.889161703431125\n",
      "23.248177952684934\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0070\n",
      "14.253811849607594\n",
      "20.821334466438174\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0056\n",
      "13.604232244622894\n",
      "19.886783886908365\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0051\n",
      "13.049417074721486\n",
      "19.101557618875447\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0044\n",
      "13.224493454718047\n",
      "19.304488468646102\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0038\n",
      "13.428698050174937\n",
      "19.585775801089483\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0036\n",
      "13.233178727520174\n",
      "19.3218383289635\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0035\n",
      "12.995466192460446\n",
      "19.00188372271123\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0036\n",
      "12.517863714352037\n",
      "18.344150923560942\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0036\n",
      "12.156821724501015\n",
      "17.848843336943368\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0035\n",
      "11.793389076983686\n",
      "17.350426941518517\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0035\n",
      "11.521655420093367\n",
      "16.977533800456552\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0035\n",
      "11.124317257126611\n",
      "16.422383747334734\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0034\n",
      "10.971282663429552\n",
      "16.20717422171889\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0034\n",
      "10.832007034337684\n",
      "16.01234523898934\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0035\n",
      "10.66606567987968\n",
      "15.780806397174873\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0035\n",
      "10.624654315644149\n",
      "15.722017533418509\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0035\n",
      "10.387225318092652\n",
      "15.381474986840047\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0034\n",
      "10.424228191962868\n",
      "15.428102457513281\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0033\n",
      "10.273493618749992\n",
      "15.21159608602673\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0034\n",
      "10.141923721765403\n",
      "15.02745080437879\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0034\n",
      "9.985954503404113\n",
      "14.80589465688299\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0033\n",
      "10.008616911549849\n",
      "14.841914081078686\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0033\n",
      "9.943910486076396\n",
      "14.749365143428548\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0033\n",
      "9.962295761892491\n",
      "14.776215101869527\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0035\n",
      "9.644163191672108\n",
      "14.325122909571787\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/100], Step [437/437], Loss: 0.0033\n",
      "9.732091360443873\n",
      "14.450974240100917\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [30/100], Step [437/437], Loss: 0.0032\n",
      "9.83241837269863\n",
      "14.593441750646612\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [31/100], Step [437/437], Loss: 0.0032\n",
      "9.782730677226787\n",
      "14.5213713227391\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [32/100], Step [437/437], Loss: 0.0032\n",
      "9.765963775148117\n",
      "14.497897548272453\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:31 batch_size64 lr:0.0001 kernel_size3 stride:1 train_acc:9.644163191672108 test_acc:14.325122909571787\n",
      "Best model parameters loaded: ./model/88lo.pth\n",
      "Best model parameters loaded../model/88lo.pth\n",
      "(Dropout: 0.4) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0562\n",
      "90.52936912605546\n",
      "132.97050350585042\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0078\n",
      "18.170731031879182\n",
      "26.535483000174924\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0065\n",
      "14.55331908098599\n",
      "21.35392405444425\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0041\n",
      "13.359089060475245\n",
      "19.551065960938264\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0030\n",
      "13.02493304447761\n",
      "19.08590703808378\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0023\n",
      "12.659187276863443\n",
      "18.61773643869813\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0020\n",
      "11.940511229177076\n",
      "17.657542907368693\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0019\n",
      "10.690013620486573\n",
      "15.893619307673355\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0021\n",
      "9.598529716956504\n",
      "14.352159837205113\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0020\n",
      "9.114362589024285\n",
      "13.689212242220398\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0020\n",
      "8.842544528348592\n",
      "13.320224344407057\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0019\n",
      "8.641975366025287\n",
      "13.044049230117377\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0017\n",
      "8.418466467363077\n",
      "12.742703777274402\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0016\n",
      "8.235626455889152\n",
      "12.50491756792897\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0015\n",
      "8.171672269832992\n",
      "12.417556993144297\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0015\n",
      "7.922821657444067\n",
      "12.085246790323508\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0013\n",
      "8.033776196644753\n",
      "12.243981089799885\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0013\n",
      "7.891744472802131\n",
      "12.051893795883757\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0013\n",
      "7.809356135311922\n",
      "11.937896747907315\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0012\n",
      "7.942051833521691\n",
      "12.128588058463496\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0011\n",
      "8.135088437335646\n",
      "12.399591949685648\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0011\n",
      "7.923889342617995\n",
      "12.10789728628896\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0010\n",
      "7.816829336440556\n",
      "11.964347264413542\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:22 batch_size64 lr:0.0001 kernel_size3 stride:1 train_acc:7.809356135311922 test_acc:11.937896747907315\n",
      "Best model parameters loaded: ./model/89lo.pth\n",
      "Best model parameters loaded../model/89lo.pth\n",
      "(Dropout: 0.4) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0577\n",
      "63.51846417366748\n",
      "93.89197552528915\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0137\n",
      "17.824382949060745\n",
      "26.384546492786566\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0132\n",
      "14.136837520342716\n",
      "20.736324680488433\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0105\n",
      "12.895654463450994\n",
      "18.911788354947834\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0084\n",
      "12.587690370707993\n",
      "18.54650161961309\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0079\n",
      "11.677021136660182\n",
      "17.31359093572645\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0069\n",
      "12.06253468759956\n",
      "17.902157254646216\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0060\n",
      "11.78446284713051\n",
      "17.524485263964163\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0055\n",
      "10.83613514720423\n",
      "16.160513584994415\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0053\n",
      "9.995794859429546\n",
      "14.953515986352835\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0049\n",
      "9.555014422399276\n",
      "14.329093480280772\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0045\n",
      "9.102805028343502\n",
      "13.681402670893117\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0043\n",
      "8.82989650664115\n",
      "13.287649228229984\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0041\n",
      "8.500461928733754\n",
      "12.81067248213499\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0039\n",
      "8.374806781004343\n",
      "12.629417749172967\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0037\n",
      "8.151044543974953\n",
      "12.300771523563343\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0035\n",
      "8.005551612176815\n",
      "12.088908162386051\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0034\n",
      "7.952649537829482\n",
      "12.019438245536032\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0033\n",
      "7.877211150786774\n",
      "11.916097266263892\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0032\n",
      "7.802091496978863\n",
      "11.802984364888903\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0031\n",
      "7.8212841304388405\n",
      "11.831441717516828\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0031\n",
      "7.741081159498241\n",
      "11.718476708197384\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0030\n",
      "7.703924596259477\n",
      "11.670577791297358\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0029\n",
      "7.656062806538841\n",
      "11.609314258596642\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0028\n",
      "7.562787963538867\n",
      "11.477595321367339\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0027\n",
      "7.553981200105491\n",
      "11.475011928826932\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0027\n",
      "7.582146754719553\n",
      "11.512511571896184\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0026\n",
      "7.533831117017066\n",
      "11.447722868773706\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/100], Step [437/437], Loss: 0.0026\n",
      "7.54539298931849\n",
      "11.463656310081529\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [30/100], Step [437/437], Loss: 0.0025\n",
      "7.521983905578792\n",
      "11.429716805789946\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/100], Step [437/437], Loss: 0.0024\n",
      "7.506649631603737\n",
      "11.409835580958909\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/100], Step [437/437], Loss: 0.0024\n",
      "7.568629027888445\n",
      "11.49770007990442\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [33/100], Step [437/437], Loss: 0.0023\n",
      "7.606797280236402\n",
      "11.55007177569592\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [34/100], Step [437/437], Loss: 0.0023\n",
      "7.618005866121866\n",
      "11.568572859849656\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [35/100], Step [437/437], Loss: 0.0023\n",
      "7.610836693182029\n",
      "11.557716773633672\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:34 batch_size64 lr:0.0001 kernel_size5 stride:1 train_acc:7.506649631603737 test_acc:11.409835580958909\n",
      "Best model parameters loaded: ./model/90lo.pth\n",
      "Best model parameters loaded../model/90lo.pth\n",
      "(Dropout: 0.4) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0363\n",
      "63.484382901266414\n",
      "93.87232644986327\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0128\n",
      "16.001425145592243\n",
      "23.68232041387962\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0100\n",
      "13.867480387890446\n",
      "20.505584852449314\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0075\n",
      "13.594943415050698\n",
      "20.201485328287248\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0065\n",
      "11.941803353498473\n",
      "17.852590158639412\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0055\n",
      "11.415792596991688\n",
      "17.071973554717307\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0047\n",
      "10.052167617074703\n",
      "15.115165079162953\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0042\n",
      "8.704861721530538\n",
      "13.205458544700456\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0038\n",
      "8.039573839259702\n",
      "12.269901722906994\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0034\n",
      "7.770722345956218\n",
      "11.88704694800083\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0030\n",
      "7.576290732427534\n",
      "11.619048652069836\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0026\n",
      "7.303809304825727\n",
      "11.240744727333334\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0023\n",
      "6.911417618318521\n",
      "10.716721413896162\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0020\n",
      "6.451099526200145\n",
      "10.099616523074394\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0017\n",
      "6.464587450974004\n",
      "10.122003540400122\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0014\n",
      "6.353759817483795\n",
      "9.984943436069797\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0013\n",
      "5.952523107289018\n",
      "9.457042022023714\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0011\n",
      "5.8393297335828\n",
      "9.297889530870528\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0010\n",
      "5.802957983107691\n",
      "9.24877663821424\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0009\n",
      "5.750909619501617\n",
      "9.171485540489474\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0008\n",
      "5.7577984758622485\n",
      "9.180133317553082\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0007\n",
      "5.61864151885311\n",
      "8.995150307230418\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0007\n",
      "5.612769659501926\n",
      "8.98716785325902\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0006\n",
      "5.479957037978829\n",
      "8.809650765034114\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0006\n",
      "5.487544519150876\n",
      "8.818067990145709\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0005\n",
      "5.613695947940113\n",
      "8.98679580415299\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0005\n",
      "5.665311220762957\n",
      "9.051016980607583\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0005\n",
      "5.71500764028762\n",
      "9.112244722863123\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:27 batch_size64 lr:0.0001 kernel_size5 stride:1 train_acc:5.479957037978829 test_acc:8.809650765034114\n",
      "Best model parameters loaded: ./model/91lo.pth\n",
      "Best model parameters loaded../model/91lo.pth\n",
      "(Dropout: 0.4) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0267\n",
      "49.64537304362578\n",
      "73.39608560172562\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0119\n",
      "17.200718741498758\n",
      "25.713039590682392\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0078\n",
      "13.910851225218414\n",
      "20.853990449367075\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0068\n",
      "11.850198797669252\n",
      "17.807778396335912\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0052\n",
      "12.1138966338852\n",
      "18.01402383981893\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0047\n",
      "10.95679491406332\n",
      "16.313258307573037\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0043\n",
      "10.606555187386535\n",
      "15.768416932101346\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0036\n",
      "10.080729631326015\n",
      "14.99786860092694\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0032\n",
      "9.552762610287719\n",
      "14.241575066021605\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0028\n",
      "9.110809129682298\n",
      "13.614608067544603\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0025\n",
      "8.650959530878652\n",
      "12.968248859389806\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0022\n",
      "8.535687903720866\n",
      "12.799387807334247\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0021\n",
      "8.57879655368499\n",
      "12.850050845398087\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0019\n",
      "8.437378675615687\n",
      "12.65188277883815\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0018\n",
      "8.336612083097908\n",
      "12.508081827929773\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0017\n",
      "8.347000197155714\n",
      "12.517134164792378\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0017\n",
      "8.259525376367801\n",
      "12.388337074024902\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0016\n",
      "8.35309790362357\n",
      "12.508963770850862\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0016\n",
      "8.183175954152034\n",
      "12.272882341303372\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0016\n",
      "8.091242422972357\n",
      "12.13276825842052\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0015\n",
      "8.144590504591681\n",
      "12.203909544009663\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0015\n",
      "8.17357571657461\n",
      "12.24460578132652\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0015\n",
      "8.035074113238798\n",
      "12.047575129631255\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0015\n",
      "8.08327110366902\n",
      "12.112560698290954\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0014\n",
      "8.21128049628552\n",
      "12.29694097616935\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0014\n",
      "8.030602123748388\n",
      "12.041285155114792\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0014\n",
      "7.895553524103681\n",
      "11.844548048690548\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0014\n",
      "7.857935054008919\n",
      "11.791814775184019\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/100], Step [437/437], Loss: 0.0013\n",
      "7.78563746897229\n",
      "11.689297042121654\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/100], Step [437/437], Loss: 0.0013\n",
      "7.700679630085134\n",
      "11.569304871822778\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/100], Step [437/437], Loss: 0.0013\n",
      "7.679153725917165\n",
      "11.53547117546297\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/100], Step [437/437], Loss: 0.0014\n",
      "7.94966503158201\n",
      "11.919320701147896\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [33/100], Step [437/437], Loss: 0.0014\n",
      "7.857541090652415\n",
      "11.781845145469177\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [34/100], Step [437/437], Loss: 0.0013\n",
      "7.93306718802109\n",
      "11.889292195932658\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [35/100], Step [437/437], Loss: 0.0014\n",
      "7.946438139635658\n",
      "11.905794260983495\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:34 batch_size64 lr:0.0001 kernel_size7 stride:1 train_acc:7.679153725917165 test_acc:11.53547117546297\n",
      "Best model parameters loaded: ./model/92lo.pth\n",
      "Best model parameters loaded../model/92lo.pth\n",
      "(Dropout: 0.4) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0621\n",
      "48.04429299423359\n",
      "70.83115464243973\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0103\n",
      "15.804255835857905\n",
      "23.50645246624727\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0058\n",
      "12.708935788851392\n",
      "18.917628527737943\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0039\n",
      "11.075584397647718\n",
      "16.5925768107888\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0033\n",
      "10.12589526820794\n",
      "15.185328139494345\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0026\n",
      "9.580452321201761\n",
      "14.36912257230777\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0021\n",
      "9.010685424266343\n",
      "13.581618703498604\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0014\n",
      "8.940876378681724\n",
      "13.478117939680418\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0009\n",
      "8.471114132107916\n",
      "12.819871456777143\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0006\n",
      "7.866018214065319\n",
      "11.976881834026196\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0004\n",
      "7.375171836318146\n",
      "11.308965256226278\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0003\n",
      "6.8975926762066235\n",
      "10.670697858883596\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0002\n",
      "6.416565244432443\n",
      "10.011554131280796\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0001\n",
      "6.201684251295014\n",
      "9.72984570944628\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0001\n",
      "5.940103098036215\n",
      "9.378722755429424\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0001\n",
      "5.775260503699012\n",
      "9.147626444038032\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0001\n",
      "5.6492665787784855\n",
      "8.968653523891634\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0001\n",
      "5.481246109416952\n",
      "8.745558325816253\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0001\n",
      "5.322732277363847\n",
      "8.538945866245543\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0001\n",
      "5.1963388085151605\n",
      "8.36233732748854\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0001\n",
      "5.123279560178315\n",
      "8.268869362132595\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0001\n",
      "4.952411126444631\n",
      "8.04949879017569\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0001\n",
      "4.861460581122178\n",
      "7.934311209819777\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0001\n",
      "4.783089861806012\n",
      "7.835855773291172\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0001\n",
      "4.691240668454142\n",
      "7.727813671482202\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0001\n",
      "4.707330236588506\n",
      "7.756765883360437\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0001\n",
      "4.798232192075633\n",
      "7.865082669205094\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0000\n",
      "4.750054913379068\n",
      "7.8120430429521575\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [29/100], Step [437/437], Loss: 0.0001\n",
      "4.720669284294256\n",
      "7.783905209267487\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:28 batch_size64 lr:0.0001 kernel_size7 stride:1 train_acc:4.691240668454142 test_acc:7.727813671482202\n",
      "Best model parameters loaded: ./model/93lo.pth\n",
      "Best model parameters loaded../model/93lo.pth\n",
      "(Dropout: 0.5) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0886\n",
      "115.54499024455556\n",
      "170.1198410529942\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0138\n",
      "25.01740306043996\n",
      "36.33339744174274\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0096\n",
      "17.987142415681674\n",
      "26.14799457622927\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0077\n",
      "15.40391093911458\n",
      "22.47223231242042\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0070\n",
      "14.195258843594788\n",
      "20.72279139195491\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0059\n",
      "13.692656088299861\n",
      "20.00527426499026\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0052\n",
      "13.43742551941712\n",
      "19.617771448144385\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0048\n",
      "13.434538963945295\n",
      "19.610866069720764\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0041\n",
      "13.727256655896964\n",
      "20.036545907600907\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0040\n",
      "13.278882582184506\n",
      "19.406659994610855\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0040\n",
      "12.811555362883794\n",
      "18.74723847318954\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0040\n",
      "12.680261915326774\n",
      "18.568796029253278\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0040\n",
      "12.576773973397865\n",
      "18.434833077457384\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0039\n",
      "12.47052034106247\n",
      "18.285846122159104\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0038\n",
      "12.327458096180356\n",
      "18.092473577763315\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0037\n",
      "12.064595879966829\n",
      "17.72424434510278\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0035\n",
      "11.840152035980653\n",
      "17.406276942351617\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0034\n",
      "11.669882632286944\n",
      "17.16641369332907\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0033\n",
      "11.36295832869464\n",
      "16.727891265132275\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0032\n",
      "11.157732197866503\n",
      "16.44167083571268\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0032\n",
      "10.986433620517724\n",
      "16.200807004582817\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0032\n",
      "10.690511599961514\n",
      "15.776351828046508\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0031\n",
      "10.651525534713718\n",
      "15.728041464487761\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0032\n",
      "10.478331549333674\n",
      "15.489496282342165\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0031\n",
      "10.366314008466869\n",
      "15.334384134191916\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0030\n",
      "10.244407493962894\n",
      "15.161912961718915\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0030\n",
      "10.216430288521012\n",
      "15.122629411209614\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0030\n",
      "10.12152834988447\n",
      "14.984853893429811\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/100], Step [437/437], Loss: 0.0030\n",
      "10.054291401971373\n",
      "14.891971163294093\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/100], Step [437/437], Loss: 0.0029\n",
      "9.90852057290387\n",
      "14.684806105678515\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/100], Step [437/437], Loss: 0.0029\n",
      "9.815398390172975\n",
      "14.55091884836519\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/100], Step [437/437], Loss: 0.0029\n",
      "9.81271063261964\n",
      "14.54337397821737\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/100], Step [437/437], Loss: 0.0029\n",
      "9.746529525506224\n",
      "14.446775572419629\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/100], Step [437/437], Loss: 0.0029\n",
      "9.672890684486243\n",
      "14.338938231443285\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/100], Step [437/437], Loss: 0.0029\n",
      "9.655213363302076\n",
      "14.31487005664074\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [36/100], Step [437/437], Loss: 0.0028\n",
      "9.607170626798196\n",
      "14.24306367714512\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [37/100], Step [437/437], Loss: 0.0028\n",
      "9.550294613159787\n",
      "14.160857024635602\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [38/100], Step [437/437], Loss: 0.0028\n",
      "9.477236735498566\n",
      "14.051708307672962\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [39/100], Step [437/437], Loss: 0.0028\n",
      "9.46469217382403\n",
      "14.033454094752852\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [40/100], Step [437/437], Loss: 0.0028\n",
      "9.328397712906309\n",
      "13.834654634950148\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [41/100], Step [437/437], Loss: 0.0028\n",
      "9.397461706703009\n",
      "13.930860377232928\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [42/100], Step [437/437], Loss: 0.0028\n",
      "9.328972867459925\n",
      "13.83145007184937\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [43/100], Step [437/437], Loss: 0.0028\n",
      "9.264634787475238\n",
      "13.733145709884193\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [44/100], Step [437/437], Loss: 0.0027\n",
      "9.229182941867728\n",
      "13.678155234518385\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [45/100], Step [437/437], Loss: 0.0028\n",
      "9.216657580830795\n",
      "13.660043746936095\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [46/100], Step [437/437], Loss: 0.0028\n",
      "9.148802860854277\n",
      "13.560022901064432\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [47/100], Step [437/437], Loss: 0.0028\n",
      "9.111551669255187\n",
      "13.503841349639172\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [48/100], Step [437/437], Loss: 0.0028\n",
      "9.042448869818676\n",
      "13.40238824947733\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [49/100], Step [437/437], Loss: 0.0028\n",
      "8.97958167951322\n",
      "13.31054686865693\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [50/100], Step [437/437], Loss: 0.0027\n",
      "9.009636623250685\n",
      "13.351098730720834\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [51/100], Step [437/437], Loss: 0.0027\n",
      "8.935762251440455\n",
      "13.24294636485839\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [52/100], Step [437/437], Loss: 0.0027\n",
      "8.938622835890198\n",
      "13.250196946940317\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [53/100], Step [437/437], Loss: 0.0027\n",
      "8.882542895768463\n",
      "13.166623358440964\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [54/100], Step [437/437], Loss: 0.0027\n",
      "8.970831981065352\n",
      "13.294458844310375\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [55/100], Step [437/437], Loss: 0.0027\n",
      "8.890355034433723\n",
      "13.17661718589318\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [56/100], Step [437/437], Loss: 0.0027\n",
      "8.88506110389982\n",
      "13.166587396916094\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [57/100], Step [437/437], Loss: 0.0027\n",
      "8.848866220671828\n",
      "13.112349862056474\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [58/100], Step [437/437], Loss: 0.0027\n",
      "8.883219389197633\n",
      "13.16216218622866\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [59/100], Step [437/437], Loss: 0.0027\n",
      "8.793795838132356\n",
      "13.034369204548895\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [60/100], Step [437/437], Loss: 0.0027\n",
      "8.83260051359628\n",
      "13.088228300119773\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [61/100], Step [437/437], Loss: 0.0027\n",
      "8.743854086707136\n",
      "12.960570382201908\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [62/100], Step [437/437], Loss: 0.0027\n",
      "8.738910357810148\n",
      "12.952995527215366\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [63/100], Step [437/437], Loss: 0.0027\n",
      "8.720639955021571\n",
      "12.925723350941972\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [64/100], Step [437/437], Loss: 0.0027\n",
      "8.693389256282932\n",
      "12.886474027659144\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [65/100], Step [437/437], Loss: 0.0027\n",
      "8.738098864585012\n",
      "12.952533558290243\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [66/100], Step [437/437], Loss: 0.0027\n",
      "8.73290705782585\n",
      "12.945072099161072\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [67/100], Step [437/437], Loss: 0.0027\n",
      "8.641845711888996\n",
      "12.813382142313413\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [68/100], Step [437/437], Loss: 0.0026\n",
      "8.73221691607313\n",
      "12.945791178564281\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [69/100], Step [437/437], Loss: 0.0027\n",
      "8.664967367601509\n",
      "12.84611851243436\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [70/100], Step [437/437], Loss: 0.0027\n",
      "8.626063440848492\n",
      "12.787501141866336\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [71/100], Step [437/437], Loss: 0.0027\n",
      "8.615852690019446\n",
      "12.77550971974164\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [72/100], Step [437/437], Loss: 0.0027\n",
      "8.598376710603537\n",
      "12.748493024664414\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [73/100], Step [437/437], Loss: 0.0026\n",
      "8.590032933032184\n",
      "12.735731709322186\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [74/100], Step [437/437], Loss: 0.0026\n",
      "8.573021176617363\n",
      "12.711794565649171\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [75/100], Step [437/437], Loss: 0.0026\n",
      "8.500067153043073\n",
      "12.60363060402557\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [76/100], Step [437/437], Loss: 0.0026\n",
      "8.557974030472103\n",
      "12.686065989805417\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [77/100], Step [437/437], Loss: 0.0026\n",
      "8.54155318679537\n",
      "12.663973814033275\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [78/100], Step [437/437], Loss: 0.0026\n",
      "8.54032320729091\n",
      "12.663072402076724\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [79/100], Step [437/437], Loss: 0.0026\n",
      "8.495320213933528\n",
      "12.59715877151918\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [80/100], Step [437/437], Loss: 0.0027\n",
      "8.471496256795595\n",
      "12.561571871758925\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [81/100], Step [437/437], Loss: 0.0026\n",
      "8.493598304615322\n",
      "12.593785324264756\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [82/100], Step [437/437], Loss: 0.0026\n",
      "8.564970372828482\n",
      "12.696469153039073\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [83/100], Step [437/437], Loss: 0.0026\n",
      "8.480549397797398\n",
      "12.572522351495556\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [84/100], Step [437/437], Loss: 0.0026\n",
      "8.465265645944665\n",
      "12.55092302541189\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [85/100], Step [437/437], Loss: 0.0026\n",
      "8.455543472324925\n",
      "12.536203755366751\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [86/100], Step [437/437], Loss: 0.0026\n",
      "8.44083328059949\n",
      "12.511512303197684\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [87/100], Step [437/437], Loss: 0.0026\n",
      "8.45513982109387\n",
      "12.529990966659245\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [88/100], Step [437/437], Loss: 0.0026\n",
      "8.426204015722671\n",
      "12.488018048391615\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [89/100], Step [437/437], Loss: 0.0026\n",
      "8.46788090757237\n",
      "12.547264782470004\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [90/100], Step [437/437], Loss: 0.0026\n",
      "8.454458690738866\n",
      "12.525505856613757\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [91/100], Step [437/437], Loss: 0.0026\n",
      "8.442854063282656\n",
      "12.506909209046677\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [92/100], Step [437/437], Loss: 0.0026\n",
      "8.479071996921553\n",
      "12.558793694395263\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:91 batch_size64 lr:0.0001 kernel_size3 stride:1 train_acc:8.426204015722671 test_acc:12.488018048391615\n",
      "Best model parameters loaded: ./model/94lo.pth\n",
      "Best model parameters loaded../model/94lo.pth\n",
      "(Dropout: 0.5) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0228\n",
      "105.92623110420678\n",
      "155.94097045715998\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0053\n",
      "20.356296560138688\n",
      "29.57173575026485\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0051\n",
      "16.029597872043535\n",
      "23.261754473121325\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0040\n",
      "13.918854352794943\n",
      "20.20185462337107\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0032\n",
      "12.975532461666527\n",
      "18.888326711039866\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0031\n",
      "12.458969422289996\n",
      "18.20879813293253\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0032\n",
      "11.730325757534104\n",
      "17.219818069627124\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0032\n",
      "11.190404786168681\n",
      "16.50423580142194\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0031\n",
      "10.649056094522226\n",
      "15.762212613735343\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0029\n",
      "9.989570254460979\n",
      "14.846615448253003\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0028\n",
      "9.315604776382939\n",
      "13.914751733851181\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0026\n",
      "8.909983314899266\n",
      "13.357255795284509\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0024\n",
      "8.596625066464293\n",
      "12.929232456712342\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0022\n",
      "8.37328298104412\n",
      "12.620541706740605\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0020\n",
      "8.152654814566823\n",
      "12.31523252008458\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0018\n",
      "7.972415042023165\n",
      "12.066052216180315\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0017\n",
      "7.779437808965723\n",
      "11.803047199711681\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0016\n",
      "7.622618730199319\n",
      "11.584174747234401\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0015\n",
      "7.464998060055119\n",
      "11.369462985707655\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0014\n",
      "7.318566576934222\n",
      "11.168364918596621\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0013\n",
      "7.214872781238649\n",
      "11.044069458584302\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0013\n",
      "7.1005217528034335\n",
      "10.888965585361671\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0012\n",
      "6.996638084472702\n",
      "10.748806359831368\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0012\n",
      "6.91204549429812\n",
      "10.632792802282328\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0011\n",
      "6.827574942771271\n",
      "10.523943774933098\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0011\n",
      "6.806872303756092\n",
      "10.490513397697557\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0010\n",
      "6.71502979390549\n",
      "10.379287669833229\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0010\n",
      "6.651763696072309\n",
      "10.304118232866662\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/100], Step [437/437], Loss: 0.0009\n",
      "6.666784816213421\n",
      "10.330232563899733\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [30/100], Step [437/437], Loss: 0.0009\n",
      "6.673207283278652\n",
      "10.35313496282178\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [31/100], Step [437/437], Loss: 0.0009\n",
      "6.518341774440889\n",
      "10.148876030101274\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/100], Step [437/437], Loss: 0.0008\n",
      "6.444761782910224\n",
      "10.057228932893373\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/100], Step [437/437], Loss: 0.0008\n",
      "6.404088443462048\n",
      "10.022617860036792\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/100], Step [437/437], Loss: 0.0008\n",
      "6.337650317644762\n",
      "9.92283453745265\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/100], Step [437/437], Loss: 0.0007\n",
      "6.331321365007352\n",
      "9.939313742592645\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [36/100], Step [437/437], Loss: 0.0007\n",
      "6.410040969361677\n",
      "10.049143718873449\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [37/100], Step [437/437], Loss: 0.0007\n",
      "6.322734601554592\n",
      "9.931748922831794\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [38/100], Step [437/437], Loss: 0.0007\n",
      "6.223875372371217\n",
      "9.801509558462836\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [39/100], Step [437/437], Loss: 0.0007\n",
      "6.270323008945471\n",
      "9.857961852243985\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [40/100], Step [437/437], Loss: 0.0007\n",
      "6.207052060273063\n",
      "9.774047000470766\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [41/100], Step [437/437], Loss: 0.0007\n",
      "6.151310698084224\n",
      "9.708028462717413\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [42/100], Step [437/437], Loss: 0.0007\n",
      "6.06096286130986\n",
      "9.571562804430584\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [43/100], Step [437/437], Loss: 0.0007\n",
      "6.087495402435014\n",
      "9.617576824873076\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [44/100], Step [437/437], Loss: 0.0007\n",
      "6.089031731009352\n",
      "9.621289109474684\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [45/100], Step [437/437], Loss: 0.0007\n",
      "5.9660684000383055\n",
      "9.453612415200363\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [46/100], Step [437/437], Loss: 0.0007\n",
      "6.056298282635463\n",
      "9.584485567280085\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [47/100], Step [437/437], Loss: 0.0007\n",
      "5.909610172778811\n",
      "9.371091074227555\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [48/100], Step [437/437], Loss: 0.0007\n",
      "5.958196952152159\n",
      "9.45506226543932\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [49/100], Step [437/437], Loss: 0.0007\n",
      "5.917475737096739\n",
      "9.386233473111405\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [50/100], Step [437/437], Loss: 0.0007\n",
      "5.908986837256729\n",
      "9.380387798383596\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [51/100], Step [437/437], Loss: 0.0007\n",
      "5.880974255630556\n",
      "9.342726546535086\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [52/100], Step [437/437], Loss: 0.0007\n",
      "5.91747305208111\n",
      "9.401688624084402\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [53/100], Step [437/437], Loss: 0.0007\n",
      "5.8172139137617815\n",
      "9.25838099363437\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [54/100], Step [437/437], Loss: 0.0007\n",
      "5.808032941524409\n",
      "9.236314019408988\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [55/100], Step [437/437], Loss: 0.0007\n",
      "5.787195043431103\n",
      "9.208858710699113\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [56/100], Step [437/437], Loss: 0.0007\n",
      "5.823802964884412\n",
      "9.274830944609699\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [57/100], Step [437/437], Loss: 0.0007\n",
      "5.72259815051672\n",
      "9.146621025529114\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [58/100], Step [437/437], Loss: 0.0007\n",
      "5.776562560007418\n",
      "9.228834454700454\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [59/100], Step [437/437], Loss: 0.0006\n",
      "5.774592614813032\n",
      "9.222419627076176\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [60/100], Step [437/437], Loss: 0.0006\n",
      "5.756515593105544\n",
      "9.206196088831055\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [61/100], Step [437/437], Loss: 0.0007\n",
      "5.691361494431959\n",
      "9.112493807677362\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [62/100], Step [437/437], Loss: 0.0007\n",
      "5.797740628268856\n",
      "9.26306510054249\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [63/100], Step [437/437], Loss: 0.0006\n",
      "5.744503521341538\n",
      "9.199966915796482\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [64/100], Step [437/437], Loss: 0.0007\n",
      "5.790346863409311\n",
      "9.254352853452536\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [65/100], Step [437/437], Loss: 0.0008\n",
      "5.718215425669692\n",
      "9.160417565459658\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:64 batch_size64 lr:0.0001 kernel_size3 stride:1 train_acc:5.691361494431959 test_acc:9.112493807677362\n",
      "Best model parameters loaded: ./model/95lo.pth\n",
      "Best model parameters loaded../model/95lo.pth\n",
      "(Dropout: 0.5) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0789\n",
      "93.28334469217502\n",
      "137.53893152363904\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0143\n",
      "19.986364357970395\n",
      "29.86566549664688\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0144\n",
      "15.317922294654156\n",
      "22.74471315081363\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0125\n",
      "13.845722768144691\n",
      "20.460390992305364\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0105\n",
      "13.850648280176454\n",
      "20.391303427555926\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0089\n",
      "13.117428222158058\n",
      "19.341609704255394\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0085\n",
      "12.411719870287808\n",
      "18.346423785735066\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0075\n",
      "12.196246629124087\n",
      "18.024918744377835\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0067\n",
      "11.175117388478066\n",
      "16.588838698890317\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0062\n",
      "10.21892181265576\n",
      "15.248645859085805\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0056\n",
      "9.540806741839102\n",
      "14.302689040282093\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0052\n",
      "9.090371747534595\n",
      "13.6728929690196\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0048\n",
      "8.843531215079787\n",
      "13.325926974861567\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0046\n",
      "8.605629108169426\n",
      "12.994607168383379\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0045\n",
      "8.437635065063091\n",
      "12.756495994227464\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0043\n",
      "8.300325225542219\n",
      "12.556856500569163\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0041\n",
      "8.137699143331059\n",
      "12.325306082146318\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0039\n",
      "7.9999966623753345\n",
      "12.123625286331473\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0038\n",
      "7.921321914390279\n",
      "11.998874273761468\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0037\n",
      "7.813698676958612\n",
      "11.833602037659086\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0036\n",
      "7.705751475872988\n",
      "11.678948275753191\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0036\n",
      "7.636410708535384\n",
      "11.570092813253842\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0034\n",
      "7.5306451762078614\n",
      "11.421468613983127\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0033\n",
      "7.494963746572219\n",
      "11.362312172971787\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0032\n",
      "7.4285316228350915\n",
      "11.26571655080441\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0031\n",
      "7.34659827113411\n",
      "11.15281271565966\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0030\n",
      "7.367174251848352\n",
      "11.181078707001198\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0029\n",
      "7.295276307155845\n",
      "11.078326551644219\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/100], Step [437/437], Loss: 0.0028\n",
      "7.2301369953939885\n",
      "10.987500689696375\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/100], Step [437/437], Loss: 0.0028\n",
      "7.2501029382413735\n",
      "11.014144926612246\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [31/100], Step [437/437], Loss: 0.0027\n",
      "7.2199282145355825\n",
      "10.965671231541533\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/100], Step [437/437], Loss: 0.0027\n",
      "7.214852023654265\n",
      "10.959858100908411\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/100], Step [437/437], Loss: 0.0027\n",
      "7.210155083626563\n",
      "10.950870683468612\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/100], Step [437/437], Loss: 0.0027\n",
      "7.1896847864050155\n",
      "10.920416877655589\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/100], Step [437/437], Loss: 0.0026\n",
      "7.197659969024054\n",
      "10.924767619083424\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [36/100], Step [437/437], Loss: 0.0026\n",
      "7.226479928266876\n",
      "10.95643846232225\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [37/100], Step [437/437], Loss: 0.0025\n",
      "7.271041527363286\n",
      "11.010111948548397\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [38/100], Step [437/437], Loss: 0.0025\n",
      "7.299773690248906\n",
      "11.036880186504556\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:37 batch_size64 lr:0.0001 kernel_size5 stride:1 train_acc:7.1896847864050155 test_acc:10.920416877655589\n",
      "Best model parameters loaded: ./model/96lo.pth\n",
      "Best model parameters loaded../model/96lo.pth\n",
      "(Dropout: 0.5) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0815\n",
      "108.11884628279554\n",
      "159.2914482710741\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0196\n",
      "22.696407664700704\n",
      "33.494349370903\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0174\n",
      "15.97621487862493\n",
      "23.594019547913703\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0138\n",
      "13.571649575660778\n",
      "19.995164893885715\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0110\n",
      "12.394810517840883\n",
      "18.294561063592273\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0085\n",
      "12.109115973287278\n",
      "17.864277867030662\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0067\n",
      "11.254404071289981\n",
      "16.67843979244229\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0062\n",
      "10.357362938090853\n",
      "15.449993872978855\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0051\n",
      "10.130125630196348\n",
      "15.155138081847891\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0040\n",
      "9.542155021664383\n",
      "14.34934660423397\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0034\n",
      "8.503089167080878\n",
      "12.912940386385383\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0030\n",
      "7.815933391303984\n",
      "11.988661005192142\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0026\n",
      "7.320340979018784\n",
      "11.328446460190355\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0024\n",
      "6.9774371841913725\n",
      "10.870789930774128\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0021\n",
      "6.665307039157898\n",
      "10.479832738207461\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0019\n",
      "6.456266168364209\n",
      "10.196382822725102\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0016\n",
      "6.246620224334452\n",
      "9.92178518662265\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0014\n",
      "6.1080638585482845\n",
      "9.735773080324709\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0012\n",
      "5.957897663684939\n",
      "9.539200852069056\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0011\n",
      "5.8433126762909025\n",
      "9.393526026250905\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0009\n",
      "5.713478392762926\n",
      "9.224440789294666\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0009\n",
      "5.6330350287290525\n",
      "9.101647026753199\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0008\n",
      "5.580871480951869\n",
      "9.030245662652256\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0007\n",
      "5.503030429258508\n",
      "8.915542453871161\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0007\n",
      "5.372404910537572\n",
      "8.7441461965675\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0006\n",
      "5.35226584746821\n",
      "8.709661020506637\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0005\n",
      "5.280709485458837\n",
      "8.606506373666193\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0005\n",
      "5.103425250453715\n",
      "8.365130854928976\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/100], Step [437/437], Loss: 0.0004\n",
      "5.115072596536119\n",
      "8.373316618100054\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [30/100], Step [437/437], Loss: 0.0005\n",
      "5.009054876659446\n",
      "8.236827401334692\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/100], Step [437/437], Loss: 0.0004\n",
      "4.980901394528128\n",
      "8.205424956132592\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/100], Step [437/437], Loss: 0.0004\n",
      "4.900018240474573\n",
      "8.095879006744687\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/100], Step [437/437], Loss: 0.0004\n",
      "4.865424942291085\n",
      "8.041197419694711\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/100], Step [437/437], Loss: 0.0004\n",
      "4.863975105242338\n",
      "8.040239877869928\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/100], Step [437/437], Loss: 0.0004\n",
      "4.8018227130600275\n",
      "7.962028757142317\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [36/100], Step [437/437], Loss: 0.0004\n",
      "4.771067795296749\n",
      "7.9276267906720514\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [37/100], Step [437/437], Loss: 0.0004\n",
      "4.720505677850239\n",
      "7.860985189933807\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [38/100], Step [437/437], Loss: 0.0004\n",
      "4.666343229905232\n",
      "7.779290606421847\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [39/100], Step [437/437], Loss: 0.0004\n",
      "4.674343884853159\n",
      "7.7968151541862\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [40/100], Step [437/437], Loss: 0.0004\n",
      "4.653347319364368\n",
      "7.755707840775342\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [41/100], Step [437/437], Loss: 0.0004\n",
      "4.595675934701556\n",
      "7.666137291378886\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [42/100], Step [437/437], Loss: 0.0004\n",
      "4.597940128176557\n",
      "7.674445506898043\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [43/100], Step [437/437], Loss: 0.0004\n",
      "4.68028629203185\n",
      "7.775691720916006\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [44/100], Step [437/437], Loss: 0.0004\n",
      "4.915985981700866\n",
      "8.08588396369215\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [45/100], Step [437/437], Loss: 0.0004\n",
      "5.103055889048847\n",
      "8.32917341783721\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:44 batch_size64 lr:0.0001 kernel_size5 stride:1 train_acc:4.595675934701556 test_acc:7.666137291378886\n",
      "Best model parameters loaded: ./model/97lo.pth\n",
      "Best model parameters loaded../model/97lo.pth\n",
      "(Dropout: 0.5) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0492\n",
      "54.85553534039774\n",
      "80.85102701044772\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0156\n",
      "17.900197285894496\n",
      "26.61349268229135\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0104\n",
      "14.300212998312709\n",
      "21.217639002369562\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0084\n",
      "12.573273430498528\n",
      "18.684221526130194\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0080\n",
      "10.965445356707662\n",
      "16.381462952516923\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0064\n",
      "11.960997437462245\n",
      "17.669228142645252\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0052\n",
      "10.981571344788566\n",
      "16.26930106880708\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0047\n",
      "10.02131461749491\n",
      "14.910392133483654\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0043\n",
      "9.889603696915906\n",
      "14.703592192314252\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0039\n",
      "9.816526687283996\n",
      "14.585334840594419\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0037\n",
      "9.5792720165622\n",
      "14.2524906774224\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0034\n",
      "9.255734421403902\n",
      "13.799673422899383\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0032\n",
      "9.005328449101537\n",
      "13.448190388045393\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0030\n",
      "8.690803297901532\n",
      "13.010465392362754\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0028\n",
      "8.653349009673565\n",
      "12.957737561022421\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0027\n",
      "8.581381260383901\n",
      "12.854611400107226\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0026\n",
      "8.316683554116286\n",
      "12.482560051626008\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0025\n",
      "8.359547876858954\n",
      "12.541568656837322\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0024\n",
      "8.271595889679048\n",
      "12.418357713460392\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0023\n",
      "8.264934592527347\n",
      "12.408469794330138\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0023\n",
      "8.339444961526972\n",
      "12.514443215402766\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0022\n",
      "8.474524425796812\n",
      "12.70217760418171\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0022\n",
      "8.393614115670776\n",
      "12.585324142405337\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0022\n",
      "8.469909519800337\n",
      "12.691493628021727\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:23 batch_size64 lr:0.0001 kernel_size7 stride:1 train_acc:8.264934592527347 test_acc:12.408469794330138\n",
      "Best model parameters loaded: ./model/98lo.pth\n",
      "Best model parameters loaded../model/98lo.pth\n",
      "(Dropout: 0.5) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/100], Step [437/437], Loss: 0.0376\n",
      "75.83104024675221\n",
      "111.74737085991595\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/100], Step [437/437], Loss: 0.0218\n",
      "24.066304219961005\n",
      "35.84779744117862\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/100], Step [437/437], Loss: 0.0166\n",
      "17.154618099485514\n",
      "25.354175688835323\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/100], Step [437/437], Loss: 0.0121\n",
      "14.751748956924992\n",
      "21.742608051559863\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/100], Step [437/437], Loss: 0.0090\n",
      "13.590162033486077\n",
      "20.032355592846244\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/100], Step [437/437], Loss: 0.0077\n",
      "12.007148988894233\n",
      "17.75852365326315\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/100], Step [437/437], Loss: 0.0063\n",
      "12.166192925371307\n",
      "17.95128508901787\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [8/100], Step [437/437], Loss: 0.0051\n",
      "11.92582421606867\n",
      "17.579288841104706\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/100], Step [437/437], Loss: 0.0043\n",
      "11.055370491393068\n",
      "16.347533761721547\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/100], Step [437/437], Loss: 0.0036\n",
      "9.961649609410554\n",
      "14.826619218777058\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/100], Step [437/437], Loss: 0.0029\n",
      "9.13942807190869\n",
      "13.701153178857576\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/100], Step [437/437], Loss: 0.0023\n",
      "8.38319090351414\n",
      "12.668837731526555\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/100], Step [437/437], Loss: 0.0019\n",
      "7.93096491217661\n",
      "12.063923688686703\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/100], Step [437/437], Loss: 0.0014\n",
      "7.535706961640983\n",
      "11.542116060536593\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/100], Step [437/437], Loss: 0.0011\n",
      "7.221701902544118\n",
      "11.116818922652996\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/100], Step [437/437], Loss: 0.0008\n",
      "6.723397602184288\n",
      "10.459628228968226\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/100], Step [437/437], Loss: 0.0007\n",
      "6.393485728049301\n",
      "10.026147809031713\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/100], Step [437/437], Loss: 0.0005\n",
      "6.148883216242447\n",
      "9.708189995764531\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/100], Step [437/437], Loss: 0.0004\n",
      "5.921424432823996\n",
      "9.416192085518196\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/100], Step [437/437], Loss: 0.0004\n",
      "5.78443722795226\n",
      "9.236617522230498\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/100], Step [437/437], Loss: 0.0003\n",
      "5.608042292798902\n",
      "9.017159054033945\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/100], Step [437/437], Loss: 0.0003\n",
      "5.49188581987285\n",
      "8.853789149204697\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/100], Step [437/437], Loss: 0.0002\n",
      "5.366532426909531\n",
      "8.696760837724538\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/100], Step [437/437], Loss: 0.0002\n",
      "5.242379104776359\n",
      "8.526724466752349\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/100], Step [437/437], Loss: 0.0002\n",
      "5.202328634891671\n",
      "8.468744550866617\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/100], Step [437/437], Loss: 0.0002\n",
      "5.071991412178915\n",
      "8.29341968419638\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/100], Step [437/437], Loss: 0.0001\n",
      "4.9983788779794756\n",
      "8.194947285935188\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/100], Step [437/437], Loss: 0.0001\n",
      "4.9729736072888295\n",
      "8.150341151613494\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/100], Step [437/437], Loss: 0.0001\n",
      "4.90608861939418\n",
      "8.070726300502692\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/100], Step [437/437], Loss: 0.0001\n",
      "4.866128183302929\n",
      "8.02451555104439\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/100], Step [437/437], Loss: 0.0001\n",
      "4.776673368518157\n",
      "7.921567852154867\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/100], Step [437/437], Loss: 0.0001\n",
      "4.733257268005164\n",
      "7.88323047152844\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/100], Step [437/437], Loss: 0.0001\n",
      "4.667427973558689\n",
      "7.787867160761392\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/100], Step [437/437], Loss: 0.0000\n",
      "4.651286912692437\n",
      "7.78667826372427\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/100], Step [437/437], Loss: 0.0000\n",
      "4.615837906469935\n",
      "7.729760427497259\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [36/100], Step [437/437], Loss: 0.0000\n",
      "4.548403824102361\n",
      "7.650813701336537\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [37/100], Step [437/437], Loss: 0.0000\n",
      "4.529036286450544\n",
      "7.610020917083152\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [38/100], Step [437/437], Loss: 0.0001\n",
      "4.478178548730975\n",
      "7.546901422066908\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [39/100], Step [437/437], Loss: 0.0000\n",
      "4.5070705342184505\n",
      "7.579827979001515\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [40/100], Step [437/437], Loss: 0.0000\n",
      "4.427326525824782\n",
      "7.474227065822724\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [41/100], Step [437/437], Loss: 0.0000\n",
      "4.378682666450015\n",
      "7.4148896916140545\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [42/100], Step [437/437], Loss: 0.0000\n",
      "4.382185262699273\n",
      "7.409219368758984\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [43/100], Step [437/437], Loss: 0.0000\n",
      "4.31862985681462\n",
      "7.328520450964414\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [44/100], Step [437/437], Loss: 0.0002\n",
      "4.933548423580304\n",
      "8.099023930761724\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [45/100], Step [437/437], Loss: 0.0002\n",
      "4.337056317983192\n",
      "7.324515753303504\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [46/100], Step [437/437], Loss: 0.0001\n",
      "4.185306480660615\n",
      "7.107306175699213\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [47/100], Step [437/437], Loss: 0.0001\n",
      "4.319681906148782\n",
      "7.271026651314775\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [48/100], Step [437/437], Loss: 0.0001\n",
      "4.252123561038301\n",
      "7.188677012545462\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [49/100], Step [437/437], Loss: 0.0001\n",
      "4.110359998533731\n",
      "7.01278890350915\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [50/100], Step [437/437], Loss: 0.0001\n",
      "4.156051816810436\n",
      "7.091592470664919\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [51/100], Step [437/437], Loss: 0.0001\n",
      "4.068623652276059\n",
      "6.98260541193643\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [52/100], Step [437/437], Loss: 0.0000\n",
      "4.523690673810513\n",
      "7.568042355198117\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [53/100], Step [437/437], Loss: 0.0000\n",
      "4.647341878555096\n",
      "7.723647003845526\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [54/100], Step [437/437], Loss: 0.0000\n",
      "4.435776658409578\n",
      "7.430863858632178\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [55/100], Step [437/437], Loss: 0.0000\n",
      "4.185779150446801\n",
      "7.1185376599362575\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:54 batch_size64 lr:0.0001 kernel_size7 stride:1 train_acc:4.068623652276059 test_acc:6.98260541193643\n",
      "Best model parameters loaded: ./model/99lo.pth\n",
      "Best model parameters loaded../model/99lo.pth\n"
     ]
    }
   ],
   "source": [
    "# Longitude+Latitude Regressor\n",
    "dropout = [0.3,0.4,0.5]\n",
    "lrs = [0.0001]\n",
    "kernel_sizes = [3,5,7]\n",
    "strides = [1]\n",
    "model_order = 82\n",
    "weight_decays = [0.001, 0.0001]\n",
    "for val in dropout:\n",
    "    for lr in lrs:\n",
    "        for kernel_size in kernel_sizes:\n",
    "            for stride in strides:\n",
    "                for weight_decay in weight_decays:\n",
    "                    padding = 0\n",
    "                    print(f'(Dropout: {val}) (kernel: {kernel_sizes}) (padding: {padding}) (lr:{lr})')\n",
    "                    cnn = CNNRegressor(n_targets=1, dropout=val, kernel_size=kernel_size, stride = stride, padding=padding) # Longitude + Latitude are targets\n",
    "                    cnn.fit(X_train_lo_cnn, y_train_lo_cnn, X_test_lo_cnn, y_test_lo_cnn)\n",
    "                    # cnn.fit(X_trainCo_cnn, y_trainCo_cnn_scaled, X_testCo_cnn, y_testCo_cnn_scaled)\n",
    "                    # cnn.train(num_epochs=100,eval_train=True, min_max_dist=min_max_dist, lr=0.0001, batch_size=16)\n",
    "                    cnn.train_model(num_epochs=100,eval_train=True, min_max_dist=min_max_dist, lr=lr, batch_size=64, patience = 4, sava_model_name=\"./model/\"+str(model_order)+\"lo.pth\",kernel_size = kernel_size, stride = stride, weight_decay=weight_decay, dropout= val)\n",
    "                    model_order += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-22T12:51:45.785681Z",
     "start_time": "2025-08-21T07:47:39.532170Z"
    }
   },
   "id": "70e798dd8e47bd6b",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Dropout: 0.3) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0234\n",
      "90.48350378544362\n",
      "131.78723400573008\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0037\n",
      "16.250598660076847\n",
      "23.84633118448337\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0029\n",
      "13.601868343670837\n",
      "20.033545028613226\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0020\n",
      "12.864389718415147\n",
      "18.969736104428055\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0014\n",
      "12.37955746383874\n",
      "18.248286137783644\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0012\n",
      "11.492623144477637\n",
      "16.960575996015873\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0014\n",
      "10.805288858248211\n",
      "15.993691150536744\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0015\n",
      "10.454304925029865\n",
      "15.501685519581933\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0016\n",
      "10.19486004500364\n",
      "15.142459971840088\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0016\n",
      "10.003559432576003\n",
      "14.875726634269347\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0016\n",
      "9.843330213016502\n",
      "14.646406178029933\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0016\n",
      "9.737981985883373\n",
      "14.495818283564684\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0017\n",
      "9.633866169446275\n",
      "14.353728564050783\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0018\n",
      "9.5175565776485\n",
      "14.192503943711898\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0017\n",
      "9.43941687346622\n",
      "14.07964451413315\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0017\n",
      "9.37140035510514\n",
      "13.980931437510366\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0016\n",
      "9.313776106471803\n",
      "13.89902303845621\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0018\n",
      "9.252467738461505\n",
      "13.813902604078843\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0018\n",
      "9.220653776503784\n",
      "13.769805325343311\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0018\n",
      "9.165378975957177\n",
      "13.68762518012386\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0017\n",
      "9.13528891879395\n",
      "13.644915482598075\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0017\n",
      "9.102464460304864\n",
      "13.599268430245663\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0017\n",
      "9.079067720389281\n",
      "13.567520574174342\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0017\n",
      "9.062324681307851\n",
      "13.542067112771894\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0017\n",
      "9.05204188806956\n",
      "13.521121831451222\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0016\n",
      "9.02898026738904\n",
      "13.48861521395429\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0016\n",
      "9.00643262399135\n",
      "13.454532207781307\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0016\n",
      "8.985206539336966\n",
      "13.41946608110273\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0016\n",
      "8.976322835758419\n",
      "13.40950737178211\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0015\n",
      "8.983291084515841\n",
      "13.42319331167588\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0015\n",
      "8.956154356281706\n",
      "13.383047093069896\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0015\n",
      "8.945552409701888\n",
      "13.375827863145743\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0015\n",
      "8.935680722229375\n",
      "13.363511040854858\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0015\n",
      "8.928366820376889\n",
      "13.356092020476645\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0014\n",
      "8.911771919260321\n",
      "13.332582186940108\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0014\n",
      "8.902910675008757\n",
      "13.32230504326041\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [37/300], Step [437/437], Loss: 0.0014\n",
      "8.90125067060008\n",
      "13.321319197808785\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [38/300], Step [437/437], Loss: 0.0014\n",
      "8.886032263646877\n",
      "13.300535095671357\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [39/300], Step [437/437], Loss: 0.0014\n",
      "8.889896775616599\n",
      "13.304404180084433\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [40/300], Step [437/437], Loss: 0.0014\n",
      "8.876296987096842\n",
      "13.287474360982793\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [41/300], Step [437/437], Loss: 0.0014\n",
      "8.886023525981084\n",
      "13.301251379300243\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [42/300], Step [437/437], Loss: 0.0014\n",
      "8.865793792948557\n",
      "13.274848862352043\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [43/300], Step [437/437], Loss: 0.0013\n",
      "8.869963757805817\n",
      "13.282819111998785\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [44/300], Step [437/437], Loss: 0.0013\n",
      "8.861685719614854\n",
      "13.274762869131091\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [45/300], Step [437/437], Loss: 0.0013\n",
      "8.873069119263642\n",
      "13.289741859544874\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [46/300], Step [437/437], Loss: 0.0013\n",
      "8.86598565902855\n",
      "13.282684878954166\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [47/300], Step [437/437], Loss: 0.0013\n",
      "8.862249842901495\n",
      "13.276272010580964\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [48/300], Step [437/437], Loss: 0.0013\n",
      "8.874627856312735\n",
      "13.295032714666881\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:47 batch_size64 lr:0.0001 kernel_size3 stride:1 train_acc:8.861685719614854 test_acc:13.274762869131091\n",
      "Best model parameters loaded: ./model/82la.pth\n",
      "Best model parameters loaded../model/82la.pth\n",
      "(Dropout: 0.3) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0182\n",
      "77.0257576230279\n",
      "112.24241548122542\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0043\n",
      "16.679051169389513\n",
      "24.388133540929083\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0034\n",
      "13.821980745943307\n",
      "20.31372594862634\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0024\n",
      "12.472022909441533\n",
      "18.3860858530695\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0015\n",
      "11.604600293265946\n",
      "17.16207921520492\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0012\n",
      "10.439625544827292\n",
      "15.51432517747441\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0015\n",
      "9.590793190474798\n",
      "14.349503457663976\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0017\n",
      "9.15300946315448\n",
      "13.737689770068368\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0018\n",
      "8.809303641082332\n",
      "13.259167951462658\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0018\n",
      "8.502860767090473\n",
      "12.845583242476884\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0015\n",
      "8.293131638937536\n",
      "12.571716084551944\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0012\n",
      "8.133768941701112\n",
      "12.367159397111768\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0009\n",
      "8.080953817565232\n",
      "12.314725187931154\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0008\n",
      "7.8926010541452705\n",
      "12.067981216686313\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0006\n",
      "7.864781440904528\n",
      "12.038791109105322\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0006\n",
      "7.756232347248165\n",
      "11.909007717359936\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0004\n",
      "7.489237892150586\n",
      "11.563382584111508\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0004\n",
      "7.458083535141994\n",
      "11.541784720856366\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0003\n",
      "7.403969301663036\n",
      "11.474571788985786\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0003\n",
      "7.304138173168583\n",
      "11.340159906868923\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0003\n",
      "7.253955782838323\n",
      "11.277883148818297\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0002\n",
      "7.0659078727247895\n",
      "11.014679337470238\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0002\n",
      "7.096625244916656\n",
      "11.051831024431934\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0002\n",
      "7.0081791637792294\n",
      "10.93092054997684\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0002\n",
      "6.904535354822328\n",
      "10.795374087237107\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0001\n",
      "6.821848014326997\n",
      "10.68802153068446\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0002\n",
      "6.8757638675642365\n",
      "10.774723273967608\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0001\n",
      "6.726083431899928\n",
      "10.569617770278263\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0001\n",
      "6.765505286361019\n",
      "10.62066984226584\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0001\n",
      "6.665214581571885\n",
      "10.473329388362105\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0001\n",
      "6.5356220997688315\n",
      "10.311269114904146\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0001\n",
      "6.487988372880876\n",
      "10.246321560633099\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0001\n",
      "6.600020624463468\n",
      "10.382988695096468\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0001\n",
      "6.522441514438001\n",
      "10.281963995692577\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0001\n",
      "6.570127101084627\n",
      "10.33804782563918\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0001\n",
      "6.412662851147869\n",
      "10.117074770472499\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [37/300], Step [437/437], Loss: 0.0001\n",
      "6.4357368126883285\n",
      "10.152723724695269\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [38/300], Step [437/437], Loss: 0.0001\n",
      "6.315083861717523\n",
      "9.991731548848552\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [39/300], Step [437/437], Loss: 0.0001\n",
      "6.407710608282557\n",
      "10.11744017874418\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [40/300], Step [437/437], Loss: 0.0001\n",
      "6.388168093849261\n",
      "10.094063756024202\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [41/300], Step [437/437], Loss: 0.0001\n",
      "6.216203529985393\n",
      "9.873087448770452\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [42/300], Step [437/437], Loss: 0.0001\n",
      "6.381723273227844\n",
      "10.079995903873066\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [43/300], Step [437/437], Loss: 0.0001\n",
      "6.147590014782075\n",
      "9.778793915831882\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [44/300], Step [437/437], Loss: 0.0001\n",
      "6.080153641914903\n",
      "9.696099977008524\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [45/300], Step [437/437], Loss: 0.0001\n",
      "6.223200641340504\n",
      "9.8788159259522\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [46/300], Step [437/437], Loss: 0.0001\n",
      "6.182959258765748\n",
      "9.835442311722153\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [47/300], Step [437/437], Loss: 0.0001\n",
      "6.217637840924232\n",
      "9.875820371623309\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [48/300], Step [437/437], Loss: 0.0001\n",
      "6.253246369083253\n",
      "9.92378203933295\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:47 batch_size64 lr:0.0001 kernel_size3 stride:1 train_acc:6.080153641914903 test_acc:9.696099977008524\n",
      "Best model parameters loaded: ./model/83la.pth\n",
      "Best model parameters loaded../model/83la.pth\n",
      "(Dropout: 0.3) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0216\n",
      "57.03391592691501\n",
      "83.14150733105745\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0032\n",
      "14.630238873324608\n",
      "21.623734033144537\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0030\n",
      "11.940603746601907\n",
      "17.82384005322159\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0021\n",
      "11.323120314437123\n",
      "16.949204789426243\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0016\n",
      "11.048024363215468\n",
      "16.565033459972785\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0013\n",
      "10.324907990773522\n",
      "15.543914604856466\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0012\n",
      "9.493738076673997\n",
      "14.365197252038476\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0012\n",
      "9.040369201202218\n",
      "13.726219307903786\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0012\n",
      "8.734614660360764\n",
      "13.28642656108873\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0013\n",
      "8.506467479886782\n",
      "12.96996141845613\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0013\n",
      "8.358292981290955\n",
      "12.754212109857914\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0014\n",
      "8.2520787224675\n",
      "12.605040459274543\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0014\n",
      "8.175749754809623\n",
      "12.498124749764726\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0013\n",
      "8.080580800444178\n",
      "12.364272633089831\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0012\n",
      "8.017532055041771\n",
      "12.27341882491812\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0011\n",
      "7.9637119508753855\n",
      "12.202151209883912\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0011\n",
      "7.912207423260272\n",
      "12.127356342973782\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0011\n",
      "7.856782377174591\n",
      "12.041814912586663\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0011\n",
      "7.8104453586357385\n",
      "11.980751296267224\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0011\n",
      "7.777010238799155\n",
      "11.93353308279202\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0010\n",
      "7.749023622887815\n",
      "11.895460007243285\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0010\n",
      "7.677833684076572\n",
      "11.795244149490731\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0010\n",
      "7.657267161830185\n",
      "11.765299312515092\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0009\n",
      "7.631946673003571\n",
      "11.722557174060581\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0009\n",
      "7.601354004767338\n",
      "11.67943427161357\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0008\n",
      "7.589743374293341\n",
      "11.661748532637644\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0007\n",
      "7.579047409582402\n",
      "11.652216209262328\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0007\n",
      "7.564590757832712\n",
      "11.626305997119804\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0007\n",
      "7.546268007379646\n",
      "11.59581877589925\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0007\n",
      "7.5297044927420425\n",
      "11.5709118686126\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0006\n",
      "7.514171128959471\n",
      "11.548808114474088\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0006\n",
      "7.50846065924898\n",
      "11.53349112853347\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0006\n",
      "7.514856298062104\n",
      "11.542560116474226\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0006\n",
      "7.503765727617759\n",
      "11.526202535763911\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0006\n",
      "7.483121090506072\n",
      "11.499252038257072\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0006\n",
      "7.498547105983854\n",
      "11.524390766668322\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [37/300], Step [437/437], Loss: 0.0006\n",
      "7.483649939654862\n",
      "11.504362731405113\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [38/300], Step [437/437], Loss: 0.0006\n",
      "7.504292416998668\n",
      "11.52934837920708\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [39/300], Step [437/437], Loss: 0.0005\n",
      "7.487090609106057\n",
      "11.511652635908993\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:38 batch_size64 lr:0.0001 kernel_size5 stride:1 train_acc:7.483121090506072 test_acc:11.499252038257072\n",
      "Best model parameters loaded: ./model/84la.pth\n",
      "Best model parameters loaded../model/84la.pth\n",
      "(Dropout: 0.3) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0232\n",
      "60.03030792582499\n",
      "87.48477440161301\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0041\n",
      "15.72887647981622\n",
      "23.215503143217806\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0031\n",
      "12.391818644935286\n",
      "18.42408814409505\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0016\n",
      "11.426113910223537\n",
      "17.102797338452525\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0009\n",
      "10.571125168955737\n",
      "15.927843251706712\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0006\n",
      "9.392794403243931\n",
      "14.272392967909331\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0006\n",
      "8.439261310516798\n",
      "12.91881020860919\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0006\n",
      "7.874215549590965\n",
      "12.119446571964247\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0006\n",
      "7.446582910589059\n",
      "11.488065997514207\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0005\n",
      "7.1288820692215555\n",
      "11.056351257350947\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0005\n",
      "6.856832524340425\n",
      "10.673183153088896\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0004\n",
      "6.587333275867673\n",
      "10.317779001862746\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0003\n",
      "6.361628664541889\n",
      "9.988076247570628\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0003\n",
      "6.145873553372593\n",
      "9.698027319457163\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0002\n",
      "5.935549047213823\n",
      "9.416681229883318\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0001\n",
      "5.765123327315887\n",
      "9.215507045355553\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0002\n",
      "5.633486117383835\n",
      "9.048676599876194\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0001\n",
      "5.493788323003013\n",
      "8.872520160069222\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0001\n",
      "5.430942282933626\n",
      "8.80779431082977\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0001\n",
      "5.305217312587129\n",
      "8.636325448563273\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0001\n",
      "5.283899249831608\n",
      "8.627235011910967\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0001\n",
      "5.191231810828156\n",
      "8.49532372392734\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0000\n",
      "5.126842678387947\n",
      "8.429747236297075\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0000\n",
      "5.050289831321949\n",
      "8.333462037536476\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0000\n",
      "5.037240103203779\n",
      "8.31398195026293\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0000\n",
      "4.981985956857541\n",
      "8.259798141088364\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0000\n",
      "4.9675287806877195\n",
      "8.234037448247744\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0000\n",
      "4.954616121169933\n",
      "8.21101747640358\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0000\n",
      "4.9106825303757065\n",
      "8.163254921491054\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0000\n",
      "4.8808065249429555\n",
      "8.132221910068557\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0000\n",
      "4.862963424014501\n",
      "8.117304878654085\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0000\n",
      "4.800699722558636\n",
      "8.040104637569334\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0000\n",
      "4.803720570524487\n",
      "8.056388728217636\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0000\n",
      "4.777586903481164\n",
      "8.016536271299849\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0000\n",
      "4.739509668778234\n",
      "7.968515927974734\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0000\n",
      "4.7017963295289285\n",
      "7.929640738490491\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [37/300], Step [437/437], Loss: 0.0000\n",
      "4.700610447200336\n",
      "7.913845259399089\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [38/300], Step [437/437], Loss: 0.0000\n",
      "4.638657664421386\n",
      "7.836058669549451\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [39/300], Step [437/437], Loss: 0.0000\n",
      "4.671314340574995\n",
      "7.871343013070544\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [40/300], Step [437/437], Loss: 0.0000\n",
      "4.604358266848265\n",
      "7.783845018916682\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [41/300], Step [437/437], Loss: 0.0000\n",
      "4.623031653879934\n",
      "7.792923500993857\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [42/300], Step [437/437], Loss: 0.0000\n",
      "4.605957182006369\n",
      "7.773568682637831\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [43/300], Step [437/437], Loss: 0.0000\n",
      "4.550710422483588\n",
      "7.708249960103728\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [44/300], Step [437/437], Loss: 0.0000\n",
      "4.576945633303375\n",
      "7.741705602913697\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [45/300], Step [437/437], Loss: 0.0000\n",
      "4.500478287009795\n",
      "7.644060290162596\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [46/300], Step [437/437], Loss: 0.0000\n",
      "4.484583069827259\n",
      "7.6160311202099695\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [47/300], Step [437/437], Loss: 0.0000\n",
      "4.4525594040603975\n",
      "7.572071869754085\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [48/300], Step [437/437], Loss: 0.0000\n",
      "4.417258102012182\n",
      "7.525908194922415\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [49/300], Step [437/437], Loss: 0.0000\n",
      "4.388851705326116\n",
      "7.489249311941586\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [50/300], Step [437/437], Loss: 0.0000\n",
      "4.409200259797172\n",
      "7.5202947608299295\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [51/300], Step [437/437], Loss: 0.0000\n",
      "4.365856818250155\n",
      "7.466438903744876\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [52/300], Step [437/437], Loss: 0.0000\n",
      "4.566905140984614\n",
      "7.7352977518063675\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [53/300], Step [437/437], Loss: 0.0000\n",
      "4.609987831510281\n",
      "7.796684225204876\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [54/300], Step [437/437], Loss: 0.0000\n",
      "4.58299880377199\n",
      "7.750599703459833\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [55/300], Step [437/437], Loss: 0.0000\n",
      "4.575764842759516\n",
      "7.746990794201483\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:54 batch_size64 lr:0.0001 kernel_size5 stride:1 train_acc:4.365856818250155 test_acc:7.466438903744876\n",
      "Best model parameters loaded: ./model/85la.pth\n",
      "Best model parameters loaded../model/85la.pth\n",
      "(Dropout: 0.3) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0171\n",
      "46.28445340101063\n",
      "67.78317336771202\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0035\n",
      "14.396168767167476\n",
      "21.346343408377752\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0024\n",
      "12.406676688488325\n",
      "18.503709405790445\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0018\n",
      "11.635149034973296\n",
      "17.386841072053926\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0018\n",
      "9.677680650866671\n",
      "14.619113442724151\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0018\n",
      "9.055860525899808\n",
      "13.723055037493852\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0016\n",
      "8.933287108032593\n",
      "13.541260755315198\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0012\n",
      "8.869784519623375\n",
      "13.435628372864462\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0010\n",
      "8.331747046038158\n",
      "12.696927580919903\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0009\n",
      "7.857301216158965\n",
      "12.030243820126039\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0008\n",
      "7.672273324745927\n",
      "11.7646848381656\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0008\n",
      "7.5162820358609865\n",
      "11.543634979274112\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0007\n",
      "7.41707602882126\n",
      "11.398295685051247\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0006\n",
      "7.3304913287358975\n",
      "11.267680468268388\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0006\n",
      "7.338299552357329\n",
      "11.273348233764263\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0006\n",
      "7.543119197103424\n",
      "11.555932792543299\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0006\n",
      "7.642061796389435\n",
      "11.678305960566387\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0006\n",
      "7.430984595583526\n",
      "11.376543075449826\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:17 batch_size64 lr:0.0001 kernel_size7 stride:1 train_acc:7.3304913287358975 test_acc:11.267680468268388\n",
      "Best model parameters loaded: ./model/86la.pth\n",
      "Best model parameters loaded../model/86la.pth\n",
      "(Dropout: 0.3) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0137\n",
      "56.83024280413109\n",
      "82.967608226439\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0032\n",
      "16.47818230853967\n",
      "24.502551835127406\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0012\n",
      "13.270561938934582\n",
      "19.77844056344839\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0009\n",
      "11.845947229189235\n",
      "17.742604777138084\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0008\n",
      "10.181885553528904\n",
      "15.373292426532469\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0009\n",
      "9.122447751975386\n",
      "13.871570012559177\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0008\n",
      "8.4257365225645\n",
      "12.852779958882639\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0006\n",
      "7.810739174256005\n",
      "11.966986672275588\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0005\n",
      "7.285632049544798\n",
      "11.209468110550599\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0003\n",
      "6.945407746230741\n",
      "10.71538425076907\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0003\n",
      "6.58821087605265\n",
      "10.199971691880455\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0002\n",
      "6.211280492666272\n",
      "9.686226571801901\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0001\n",
      "6.016319294044525\n",
      "9.40346081224172\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0001\n",
      "5.825791601997887\n",
      "9.135377242290671\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0001\n",
      "5.643858309011917\n",
      "8.896919853820172\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0001\n",
      "5.478745668662635\n",
      "8.681786991336594\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0001\n",
      "5.34843001855086\n",
      "8.51830236952429\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0001\n",
      "5.275059466883758\n",
      "8.402505982238473\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0000\n",
      "5.210826614742489\n",
      "8.307343374180867\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0000\n",
      "5.082581489373449\n",
      "8.148639673265002\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0000\n",
      "5.02280947259995\n",
      "8.066398608015332\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0000\n",
      "4.902045404513787\n",
      "7.925323897766555\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0000\n",
      "4.878490343230781\n",
      "7.894555390444146\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0000\n",
      "4.764626607934437\n",
      "7.759338509723535\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0000\n",
      "4.701581439486811\n",
      "7.691878434871869\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0000\n",
      "4.5826846126308896\n",
      "7.544729836988505\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0000\n",
      "4.6040615037441395\n",
      "7.5655609676979765\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0000\n",
      "4.5100882393646495\n",
      "7.467695929591346\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0000\n",
      "4.456586660787335\n",
      "7.42358075577201\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0000\n",
      "4.464507804967608\n",
      "7.439074834638139\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0000\n",
      "4.491371428147068\n",
      "7.428898208026373\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0001\n",
      "4.375506848991066\n",
      "7.3281474291375694\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0000\n",
      "4.306967761380573\n",
      "7.233632298867089\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0000\n",
      "4.207691894713607\n",
      "7.105768236489947\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0000\n",
      "4.1749767580237345\n",
      "7.025110331234812\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0000\n",
      "4.267002100622299\n",
      "7.1839598508943\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [37/300], Step [437/437], Loss: 0.0001\n",
      "4.390912222529028\n",
      "7.351578210139592\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [38/300], Step [437/437], Loss: 0.0000\n",
      "4.430424857936566\n",
      "7.414224467107787\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [39/300], Step [437/437], Loss: 0.0000\n",
      "4.402882399284867\n",
      "7.371502202580436\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:38 batch_size64 lr:0.0001 kernel_size7 stride:1 train_acc:4.1749767580237345 test_acc:7.025110331234812\n",
      "Best model parameters loaded: ./model/87la.pth\n",
      "Best model parameters loaded../model/87la.pth\n",
      "(Dropout: 0.4) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0136\n",
      "117.13967656446994\n",
      "170.38269934285879\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0053\n",
      "16.90578551760337\n",
      "24.68935971699245\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0044\n",
      "13.911407309774782\n",
      "20.39428454993881\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0033\n",
      "12.931672632854275\n",
      "18.969325569515835\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0022\n",
      "12.744860000295178\n",
      "18.699676122042572\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0019\n",
      "11.8704705339382\n",
      "17.425770032236894\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0018\n",
      "10.981252689733811\n",
      "16.13147219303864\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0018\n",
      "10.434361166858327\n",
      "15.363130760187438\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0019\n",
      "10.125783637252178\n",
      "14.94828640882592\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0021\n",
      "9.90316765982011\n",
      "14.64561846629347\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0022\n",
      "9.717498601903532\n",
      "14.397150903238947\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0023\n",
      "9.613603942283726\n",
      "14.254975574693203\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0024\n",
      "9.53761627538872\n",
      "14.151273543713621\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0025\n",
      "9.476093470188145\n",
      "14.065755416989749\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0024\n",
      "9.445624010598996\n",
      "14.020553483993934\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0024\n",
      "9.371315058385193\n",
      "13.91820588399473\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0024\n",
      "9.347349662708112\n",
      "13.881588891759328\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0024\n",
      "9.239218869626365\n",
      "13.733351111270984\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0024\n",
      "9.170062676000914\n",
      "13.64039774132201\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0024\n",
      "9.102092200590409\n",
      "13.55511752751063\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0023\n",
      "9.061943149350382\n",
      "13.497037116046023\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0023\n",
      "9.02719324448946\n",
      "13.448006060641772\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0023\n",
      "8.98587366603981\n",
      "13.387462058128618\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0023\n",
      "8.9666439941352\n",
      "13.359911151123265\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0022\n",
      "8.94680164004284\n",
      "13.330335834438596\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0022\n",
      "8.933625861876077\n",
      "13.308411997012037\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0021\n",
      "8.895379574659392\n",
      "13.254102625151416\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0021\n",
      "8.898524849978473\n",
      "13.258571496967628\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0021\n",
      "8.86527082837999\n",
      "13.210315246683347\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0020\n",
      "8.85739805567682\n",
      "13.193619432725015\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0019\n",
      "8.831608572861018\n",
      "13.156183858121338\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0019\n",
      "8.818237371945033\n",
      "13.136071939311613\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0019\n",
      "8.803044019578282\n",
      "13.112685333695227\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0018\n",
      "8.785813605421282\n",
      "13.087498244598832\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0018\n",
      "8.767086816346879\n",
      "13.0630080133284\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0018\n",
      "8.754542677627764\n",
      "13.04734936014748\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [37/300], Step [437/437], Loss: 0.0017\n",
      "8.742995790736407\n",
      "13.036631093751154\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [38/300], Step [437/437], Loss: 0.0017\n",
      "8.739320746281729\n",
      "13.031504401459792\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [39/300], Step [437/437], Loss: 0.0017\n",
      "8.724283100362186\n",
      "13.007988100412343\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [40/300], Step [437/437], Loss: 0.0017\n",
      "8.723277239069587\n",
      "13.008675521282642\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [41/300], Step [437/437], Loss: 0.0017\n",
      "8.702488925059033\n",
      "12.983518292208045\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [42/300], Step [437/437], Loss: 0.0017\n",
      "8.699097177695721\n",
      "12.97923349547834\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [43/300], Step [437/437], Loss: 0.0017\n",
      "8.687937502329767\n",
      "12.963170383166187\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [44/300], Step [437/437], Loss: 0.0017\n",
      "8.668817126857736\n",
      "12.939215619672796\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [45/300], Step [437/437], Loss: 0.0017\n",
      "8.667183556237989\n",
      "12.937009819858861\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [46/300], Step [437/437], Loss: 0.0017\n",
      "8.673565711022627\n",
      "12.946822714345062\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [47/300], Step [437/437], Loss: 0.0017\n",
      "8.648983034554897\n",
      "12.913331550385438\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [48/300], Step [437/437], Loss: 0.0016\n",
      "8.651180167165872\n",
      "12.917034076875291\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [49/300], Step [437/437], Loss: 0.0016\n",
      "8.647992827934761\n",
      "12.915026851822564\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [50/300], Step [437/437], Loss: 0.0016\n",
      "8.637422341237816\n",
      "12.90321922091608\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [51/300], Step [437/437], Loss: 0.0016\n",
      "8.636049432322176\n",
      "12.901905393419801\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [52/300], Step [437/437], Loss: 0.0016\n",
      "8.628686026269394\n",
      "12.894024595806231\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [53/300], Step [437/437], Loss: 0.0016\n",
      "8.624811313984752\n",
      "12.890331219489164\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [54/300], Step [437/437], Loss: 0.0016\n",
      "8.619752802610153\n",
      "12.88393679156833\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [55/300], Step [437/437], Loss: 0.0016\n",
      "8.617041365410566\n",
      "12.883261126944697\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [56/300], Step [437/437], Loss: 0.0016\n",
      "8.613621305789966\n",
      "12.878260012421064\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [57/300], Step [437/437], Loss: 0.0016\n",
      "8.611669506749262\n",
      "12.878375877507073\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [58/300], Step [437/437], Loss: 0.0016\n",
      "8.612814738426827\n",
      "12.87914437303405\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [59/300], Step [437/437], Loss: 0.0015\n",
      "8.612703127532656\n",
      "12.88174463714093\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [60/300], Step [437/437], Loss: 0.0016\n",
      "8.611413615661874\n",
      "12.880664623954061\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:59 batch_size64 lr:0.0001 kernel_size3 stride:1 train_acc:8.613621305789966 test_acc:12.878260012421064\n",
      "Best model parameters loaded: ./model/88la.pth\n",
      "Best model parameters loaded../model/88la.pth\n",
      "(Dropout: 0.4) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0177\n",
      "124.60796110858907\n",
      "181.23344168222147\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0041\n",
      "18.131632641685812\n",
      "26.543546345047325\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0044\n",
      "14.355695703726061\n",
      "21.13838649614917\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0033\n",
      "13.196618490638295\n",
      "19.4178218298442\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0025\n",
      "12.293701013663412\n",
      "18.083607126341597\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0023\n",
      "10.915338145358318\n",
      "16.13019698657459\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0023\n",
      "9.972841450373457\n",
      "14.79336559179874\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0023\n",
      "9.444172450162034\n",
      "14.049908759533462\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0021\n",
      "9.467130452688687\n",
      "14.09666266513086\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0014\n",
      "9.672168749979326\n",
      "14.41677647200059\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0010\n",
      "9.333325843346135\n",
      "13.959100176727022\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0009\n",
      "8.925759733147105\n",
      "13.397766986922115\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0008\n",
      "8.612984176556443\n",
      "12.97671998320917\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0007\n",
      "8.296711722234406\n",
      "12.560832079222065\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0007\n",
      "8.08251374241098\n",
      "12.27643741976766\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0006\n",
      "7.858645335101289\n",
      "11.974926028783683\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0006\n",
      "7.668814915676583\n",
      "11.715943105953958\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0005\n",
      "7.419536898712576\n",
      "11.383236646111971\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0005\n",
      "7.460947725852131\n",
      "11.452321122637036\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0004\n",
      "7.239999506114498\n",
      "11.150334086807973\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0004\n",
      "7.160393713634214\n",
      "11.035803342455635\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0003\n",
      "7.081373134137301\n",
      "10.93195689366137\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0003\n",
      "6.938881559062101\n",
      "10.741193761741009\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0003\n",
      "6.847031547921487\n",
      "10.631192192066722\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0003\n",
      "6.770006743911997\n",
      "10.533651754775173\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0003\n",
      "6.762090541853376\n",
      "10.52727988564086\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0003\n",
      "6.752298092068495\n",
      "10.525335469179911\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0003\n",
      "6.670279734600535\n",
      "10.4175053417709\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0003\n",
      "6.655972696695514\n",
      "10.403115402908192\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0002\n",
      "6.607507271185537\n",
      "10.33473197515265\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0003\n",
      "6.653345869106292\n",
      "10.412702322448686\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0002\n",
      "6.6252628432848795\n",
      "10.381710188154505\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0002\n",
      "6.61650315752188\n",
      "10.375216744739316\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0002\n",
      "6.5756348813944525\n",
      "10.325673047789564\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0002\n",
      "6.464053561991002\n",
      "10.162729333514847\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0002\n",
      "6.5048628663086285\n",
      "10.22587902879236\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [37/300], Step [437/437], Loss: 0.0002\n",
      "6.535490053540868\n",
      "10.277664968444942\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [38/300], Step [437/437], Loss: 0.0002\n",
      "6.561708780984112\n",
      "10.299249139958267\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [39/300], Step [437/437], Loss: 0.0002\n",
      "6.527059173026266\n",
      "10.254992689843316\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:38 batch_size64 lr:0.0001 kernel_size3 stride:1 train_acc:6.464053561991002 test_acc:10.162729333514847\n",
      "Best model parameters loaded: ./model/89la.pth\n",
      "Best model parameters loaded../model/89la.pth\n",
      "(Dropout: 0.4) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0331\n",
      "108.93684889397288\n",
      "158.1928226950891\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0073\n",
      "18.97675063661124\n",
      "27.83819286223725\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0049\n",
      "15.428791993879734\n",
      "22.71982794048452\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0032\n",
      "14.304684726474758\n",
      "21.116018848671548\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0024\n",
      "13.737264550460475\n",
      "20.29457679165383\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0019\n",
      "12.623307160563906\n",
      "18.71868913520276\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0018\n",
      "11.17957161839181\n",
      "16.698483618111233\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0018\n",
      "10.326909842686245\n",
      "15.497168661080305\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0019\n",
      "9.784415470968428\n",
      "14.745938496959287\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0020\n",
      "9.37518345503421\n",
      "14.175319458818397\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0021\n",
      "9.133603604394807\n",
      "13.84528628857462\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0020\n",
      "9.009397648166988\n",
      "13.663549452208452\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0019\n",
      "8.911251517144517\n",
      "13.523170268620937\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0017\n",
      "8.907482568858503\n",
      "13.514813392633942\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0016\n",
      "8.82137276496287\n",
      "13.396049497448244\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0016\n",
      "8.783821067994433\n",
      "13.339116879066118\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0015\n",
      "8.704850867528474\n",
      "13.22765290251418\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0014\n",
      "8.568590384325956\n",
      "13.035429532051767\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0015\n",
      "8.327152051465902\n",
      "12.68362277482003\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0014\n",
      "8.273038934514846\n",
      "12.603070240961246\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0013\n",
      "8.26919728581151\n",
      "12.59048780215903\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0014\n",
      "8.070425774950154\n",
      "12.302539186204816\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0015\n",
      "7.933998045596264\n",
      "12.10402171569794\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0014\n",
      "7.889855778933738\n",
      "12.03485131812183\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0014\n",
      "7.804782282806115\n",
      "11.916390181158492\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0014\n",
      "7.751614778821075\n",
      "11.838315084792779\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0014\n",
      "7.716905976766339\n",
      "11.784325504178016\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0013\n",
      "7.674404835784127\n",
      "11.717098720218884\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0013\n",
      "7.656361295955251\n",
      "11.688116412008679\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0012\n",
      "7.632271179732641\n",
      "11.649198868226145\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0012\n",
      "7.613600459804415\n",
      "11.6193971621277\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0011\n",
      "7.590335958298983\n",
      "11.582800006439145\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0011\n",
      "7.5712565505463845\n",
      "11.555810055031559\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0010\n",
      "7.557680803726823\n",
      "11.54037381294451\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0010\n",
      "7.539474020606269\n",
      "11.503895657106678\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0010\n",
      "7.531049180236394\n",
      "11.486571111957401\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [37/300], Step [437/437], Loss: 0.0010\n",
      "7.521886789394869\n",
      "11.463578885279148\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [38/300], Step [437/437], Loss: 0.0009\n",
      "7.515105658066499\n",
      "11.457443460214657\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [39/300], Step [437/437], Loss: 0.0009\n",
      "7.524230078645843\n",
      "11.467291916746769\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [40/300], Step [437/437], Loss: 0.0009\n",
      "7.5219088980615005\n",
      "11.461225026909991\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [41/300], Step [437/437], Loss: 0.0008\n",
      "7.548186801626885\n",
      "11.497988045793868\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [42/300], Step [437/437], Loss: 0.0008\n",
      "7.54069965036449\n",
      "11.485347903726902\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:41 batch_size64 lr:0.0001 kernel_size5 stride:1 train_acc:7.515105658066499 test_acc:11.457443460214657\n",
      "Best model parameters loaded: ./model/90la.pth\n",
      "Best model parameters loaded../model/90la.pth\n",
      "(Dropout: 0.4) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0166\n",
      "106.63108382684473\n",
      "155.04757429550713\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0029\n",
      "16.72329792093902\n",
      "24.664569873644997\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0028\n",
      "12.649275560890445\n",
      "18.821161154735183\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0029\n",
      "10.944124930323216\n",
      "16.42179654835281\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0018\n",
      "10.281488629108656\n",
      "15.455107106692692\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0012\n",
      "9.511348870937042\n",
      "14.367708671935459\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0008\n",
      "8.7036741273468\n",
      "13.245659246497938\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0007\n",
      "8.119329480096225\n",
      "12.443412272791363\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0006\n",
      "7.734892968411921\n",
      "11.912492621506876\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0007\n",
      "7.434521902177225\n",
      "11.469985591290609\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0006\n",
      "7.186125464882931\n",
      "11.119303504272974\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0004\n",
      "6.892192093095092\n",
      "10.723288746898554\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0003\n",
      "6.671493455703666\n",
      "10.442545053993866\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0003\n",
      "6.48381341665038\n",
      "10.196430970509011\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0002\n",
      "6.28692802590639\n",
      "9.93332136143134\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0002\n",
      "6.134891372713078\n",
      "9.738635519386685\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0001\n",
      "5.951113597801613\n",
      "9.486019517426325\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0001\n",
      "5.861475528482691\n",
      "9.368015483840967\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0001\n",
      "5.758304496286641\n",
      "9.236781289768984\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0001\n",
      "5.69529466838536\n",
      "9.146952497011934\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0001\n",
      "5.601161650190119\n",
      "9.030303994165314\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0001\n",
      "5.5331215968919185\n",
      "8.947646996619584\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0001\n",
      "5.433735987668007\n",
      "8.819506381191186\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0001\n",
      "5.434779937329645\n",
      "8.81773246521225\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0001\n",
      "5.4100364283913995\n",
      "8.79035508047995\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0001\n",
      "5.360668476600208\n",
      "8.737796670403922\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0000\n",
      "5.331327682400647\n",
      "8.69992662629325\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0000\n",
      "5.200096558496888\n",
      "8.520003980138029\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0000\n",
      "5.205231333857451\n",
      "8.525062380451264\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0000\n",
      "5.1141419699045185\n",
      "8.402233299899939\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0000\n",
      "5.102182618989404\n",
      "8.40307954795471\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0000\n",
      "5.113219730449068\n",
      "8.41199932812698\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0000\n",
      "5.051517668439134\n",
      "8.33199794086519\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0000\n",
      "4.968470779595272\n",
      "8.237755190929938\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0000\n",
      "5.002585512239983\n",
      "8.27582018881548\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0000\n",
      "4.943185242449172\n",
      "8.19843067844641\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [37/300], Step [437/437], Loss: 0.0000\n",
      "5.026429200166632\n",
      "8.310031930675413\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [38/300], Step [437/437], Loss: 0.0001\n",
      "5.116872806309684\n",
      "8.42940042223205\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [39/300], Step [437/437], Loss: 0.0000\n",
      "4.880122357311431\n",
      "8.113643143891498\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [40/300], Step [437/437], Loss: 0.0000\n",
      "4.863743435639108\n",
      "8.089213539044666\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [41/300], Step [437/437], Loss: 0.0001\n",
      "4.923141715512138\n",
      "8.173600305033876\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [42/300], Step [437/437], Loss: 0.0000\n",
      "4.833233105901641\n",
      "8.056530335014411\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [43/300], Step [437/437], Loss: 0.0000\n",
      "4.847697111037368\n",
      "8.07967029451178\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [44/300], Step [437/437], Loss: 0.0000\n",
      "4.767345980423524\n",
      "7.975935603595569\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [45/300], Step [437/437], Loss: 0.0000\n",
      "4.764514538081392\n",
      "7.976246194381022\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [46/300], Step [437/437], Loss: 0.0000\n",
      "4.7624273604107525\n",
      "7.975144482987527\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [47/300], Step [437/437], Loss: 0.0000\n",
      "4.693655069434412\n",
      "7.880059029372715\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [48/300], Step [437/437], Loss: 0.0000\n",
      "4.6896099603324926\n",
      "7.870884570478152\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [49/300], Step [437/437], Loss: 0.0000\n",
      "4.8274872550141685\n",
      "8.061643186782895\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [50/300], Step [437/437], Loss: 0.0001\n",
      "4.841266498652095\n",
      "8.068819749186735\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [51/300], Step [437/437], Loss: 0.0001\n",
      "4.913487676336444\n",
      "8.178623679070219\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [52/300], Step [437/437], Loss: 0.0001\n",
      "4.823197489042677\n",
      "8.048341399235676\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:51 batch_size64 lr:0.0001 kernel_size5 stride:1 train_acc:4.6896099603324926 test_acc:7.870884570478152\n",
      "Best model parameters loaded: ./model/91la.pth\n",
      "Best model parameters loaded../model/91la.pth\n",
      "(Dropout: 0.4) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0315\n",
      "66.7970117580406\n",
      "97.33519430108517\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0062\n",
      "16.0460592974638\n",
      "23.797812147742793\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0029\n",
      "12.73679472209115\n",
      "19.0363613856071\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0018\n",
      "11.394410167881214\n",
      "17.085015941964787\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0012\n",
      "10.552006345820422\n",
      "15.849324836415395\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0011\n",
      "9.661538276256396\n",
      "14.599283505345594\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0012\n",
      "9.185008537576694\n",
      "13.935377422277572\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0016\n",
      "8.854606131719159\n",
      "13.466997225102565\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0016\n",
      "8.511846821664944\n",
      "12.9822613792427\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0015\n",
      "8.27779468184927\n",
      "12.644770846437275\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0012\n",
      "8.200468291520247\n",
      "12.512843561715995\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0009\n",
      "8.219531060318085\n",
      "12.52718781027268\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0007\n",
      "8.159750418535124\n",
      "12.446705946390486\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0006\n",
      "7.975808167592933\n",
      "12.18919690998752\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0005\n",
      "7.7911009767352795\n",
      "11.932337134431055\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0005\n",
      "7.670923905957583\n",
      "11.764068085467624\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0004\n",
      "7.540586802888595\n",
      "11.580381319969867\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0004\n",
      "7.497694809903301\n",
      "11.52226597938017\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0004\n",
      "7.388846782901748\n",
      "11.37535300828868\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0004\n",
      "7.370212199357247\n",
      "11.346265819067389\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0003\n",
      "7.152355681913663\n",
      "11.052947364295653\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0003\n",
      "7.12555573555105\n",
      "11.015768738898071\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0003\n",
      "7.022220038502301\n",
      "10.879496950357083\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0003\n",
      "6.944660191905719\n",
      "10.775706558150846\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0003\n",
      "6.896421726565217\n",
      "10.71529077278847\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0003\n",
      "6.864196882698853\n",
      "10.672132124678633\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0003\n",
      "6.848985814222317\n",
      "10.647120701301553\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0002\n",
      "6.873231444370029\n",
      "10.676640445954355\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0002\n",
      "6.7978627151055155\n",
      "10.57421118024543\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0002\n",
      "6.824195158604831\n",
      "10.612896454709738\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0003\n",
      "6.7814401989624\n",
      "10.555385616392526\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0002\n",
      "6.732988542259704\n",
      "10.492804371465192\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0002\n",
      "6.6987561093946155\n",
      "10.446835217573998\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0002\n",
      "6.708207739143576\n",
      "10.458043886173675\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0002\n",
      "6.778877969373259\n",
      "10.545887048641092\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0002\n",
      "6.742926504586217\n",
      "10.495826393089999\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [37/300], Step [437/437], Loss: 0.0002\n",
      "6.820396346700281\n",
      "10.599470772341538\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:36 batch_size64 lr:0.0001 kernel_size7 stride:1 train_acc:6.6987561093946155 test_acc:10.446835217573998\n",
      "Best model parameters loaded: ./model/92la.pth\n",
      "Best model parameters loaded../model/92la.pth\n",
      "(Dropout: 0.4) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0470\n",
      "85.53063057091067\n",
      "124.52483468284684\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0065\n",
      "15.829011732216145\n",
      "23.356437278144334\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0043\n",
      "12.429878164175761\n",
      "18.457385918608814\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0030\n",
      "11.176943112583265\n",
      "16.668769794327336\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0021\n",
      "10.393947863336829\n",
      "15.613221125334121\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0015\n",
      "9.720129421169839\n",
      "14.710807131390405\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0013\n",
      "8.795905305560298\n",
      "13.405691447838828\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0014\n",
      "8.189301229942163\n",
      "12.547059910653033\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0015\n",
      "7.788733545014081\n",
      "11.976187602814761\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0015\n",
      "7.383882429277873\n",
      "11.399455885482507\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0012\n",
      "6.994314750345466\n",
      "10.866540331470388\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0009\n",
      "6.683150901628698\n",
      "10.4426999169223\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0006\n",
      "6.443967247695362\n",
      "10.106798231724689\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0005\n",
      "6.2095554822355545\n",
      "9.776515135748463\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0004\n",
      "5.982129754277107\n",
      "9.478277144656795\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0002\n",
      "5.832936712962419\n",
      "9.273610202369431\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0002\n",
      "5.627916710502865\n",
      "8.991923651718007\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0002\n",
      "5.46720141924164\n",
      "8.758851349762313\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0001\n",
      "5.30541711789153\n",
      "8.552008602076128\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0001\n",
      "5.189604640745777\n",
      "8.400015922482975\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0001\n",
      "5.222881796086263\n",
      "8.426729906895963\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0001\n",
      "5.106305329971264\n",
      "8.279207096220338\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0001\n",
      "5.038543950036176\n",
      "8.194681634634268\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0001\n",
      "5.013936657128271\n",
      "8.160201153457221\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0001\n",
      "4.961356623807115\n",
      "8.096756676490145\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0000\n",
      "4.973096083211447\n",
      "8.104900807871612\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0001\n",
      "4.817502906052724\n",
      "7.908474633813911\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0001\n",
      "4.931240922758481\n",
      "8.02967318863565\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0000\n",
      "5.256310613849275\n",
      "8.454771965145042\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0000\n",
      "4.482920162191482\n",
      "7.504738155528578\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0001\n",
      "4.339341829639873\n",
      "7.314904336196686\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0000\n",
      "4.269461968947576\n",
      "7.230091384718722\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0000\n",
      "4.272965147364937\n",
      "7.2333893652421475\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0001\n",
      "4.450582961490482\n",
      "7.433344865042841\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0000\n",
      "4.4756966836392875\n",
      "7.45968762579311\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0000\n",
      "4.653788748126285\n",
      "7.696489300334078\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:35 batch_size64 lr:0.0001 kernel_size7 stride:1 train_acc:4.269461968947576 test_acc:7.230091384718722\n",
      "Best model parameters loaded: ./model/93la.pth\n",
      "Best model parameters loaded../model/93la.pth\n",
      "(Dropout: 0.5) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0223\n",
      "116.07257805579005\n",
      "168.51079590844066\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0038\n",
      "21.46978738650189\n",
      "31.667551837795653\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0035\n",
      "17.33783832798176\n",
      "25.554917910557567\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0033\n",
      "15.160703065600728\n",
      "22.35428233002492\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0032\n",
      "13.852870942670824\n",
      "20.449600395747\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0032\n",
      "12.924465211648348\n",
      "19.067440488526298\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0029\n",
      "12.334500697903907\n",
      "18.1679247723418\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0023\n",
      "12.116353808454946\n",
      "17.832165313607504\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0019\n",
      "12.052975442351297\n",
      "17.740650761524247\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0017\n",
      "11.845992890139627\n",
      "17.445763786608836\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0015\n",
      "11.55436068443034\n",
      "17.03298012011302\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0014\n",
      "11.274037990421608\n",
      "16.637856852813226\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0014\n",
      "10.913350028311605\n",
      "16.125849074854827\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0014\n",
      "10.521571007872938\n",
      "15.571716210136112\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0014\n",
      "10.19836721705673\n",
      "15.118448533040493\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0015\n",
      "9.937808774351717\n",
      "14.74839378828339\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0015\n",
      "9.737474040804285\n",
      "14.46406307007197\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0016\n",
      "9.588547363891378\n",
      "14.257818485225812\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0016\n",
      "9.4744290071472\n",
      "14.102914175214538\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0016\n",
      "9.39838723990322\n",
      "14.000298432271464\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0016\n",
      "9.3238540187024\n",
      "13.899621223201656\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0016\n",
      "9.24522606525091\n",
      "13.790094852862058\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0016\n",
      "9.187040644028658\n",
      "13.710153298247409\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0016\n",
      "9.129617394200963\n",
      "13.630673898256605\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0015\n",
      "9.083212242332342\n",
      "13.565317822983868\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0015\n",
      "9.032385204456709\n",
      "13.492360925714165\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0015\n",
      "8.999609853889645\n",
      "13.44618891525814\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0015\n",
      "8.960844903915332\n",
      "13.392461934290653\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0015\n",
      "8.922594348545422\n",
      "13.342364310907108\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0015\n",
      "8.88405142499957\n",
      "13.287300201509591\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0014\n",
      "8.85418677055859\n",
      "13.244334055618362\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0015\n",
      "8.821499946141087\n",
      "13.199725460395339\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0014\n",
      "8.798384346178485\n",
      "13.169206056577316\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0014\n",
      "8.78012460648219\n",
      "13.147683772795522\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0015\n",
      "8.74753197912486\n",
      "13.104541879022307\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0014\n",
      "8.730255806381155\n",
      "13.086561043307409\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [37/300], Step [437/437], Loss: 0.0014\n",
      "8.713411819346048\n",
      "13.066831913966421\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [38/300], Step [437/437], Loss: 0.0013\n",
      "8.698315906419177\n",
      "13.04833606768254\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [39/300], Step [437/437], Loss: 0.0013\n",
      "8.689493684269033\n",
      "13.033540802724444\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [40/300], Step [437/437], Loss: 0.0014\n",
      "8.67317647312803\n",
      "13.006433572440868\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [41/300], Step [437/437], Loss: 0.0013\n",
      "8.664009207510912\n",
      "12.98631066224671\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [42/300], Step [437/437], Loss: 0.0013\n",
      "8.658645205732933\n",
      "12.976671577186524\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [43/300], Step [437/437], Loss: 0.0013\n",
      "8.646861298525149\n",
      "12.959544125464827\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [44/300], Step [437/437], Loss: 0.0013\n",
      "8.645751740892798\n",
      "12.957338952002416\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [45/300], Step [437/437], Loss: 0.0013\n",
      "8.629110314732971\n",
      "12.935921388607397\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [46/300], Step [437/437], Loss: 0.0013\n",
      "8.628015839827173\n",
      "12.929863711999552\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [47/300], Step [437/437], Loss: 0.0013\n",
      "8.613067780801268\n",
      "12.914332849241715\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [48/300], Step [437/437], Loss: 0.0013\n",
      "8.60310898903989\n",
      "12.901384773444727\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [49/300], Step [437/437], Loss: 0.0013\n",
      "8.604606222750993\n",
      "12.903211959142943\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [50/300], Step [437/437], Loss: 0.0013\n",
      "8.604765971256828\n",
      "12.903375439806053\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [51/300], Step [437/437], Loss: 0.0013\n",
      "8.584152099733021\n",
      "12.876728487543664\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [52/300], Step [437/437], Loss: 0.0012\n",
      "8.586454632979635\n",
      "12.879120658141124\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [53/300], Step [437/437], Loss: 0.0012\n",
      "8.596439587090536\n",
      "12.893209687533943\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [54/300], Step [437/437], Loss: 0.0012\n",
      "8.593438084862953\n",
      "12.88929809629425\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [55/300], Step [437/437], Loss: 0.0012\n",
      "8.585759488016961\n",
      "12.879032901198654\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:54 batch_size64 lr:0.0001 kernel_size3 stride:1 train_acc:8.584152099733021 test_acc:12.876728487543664\n",
      "Best model parameters loaded: ./model/94la.pth\n",
      "Best model parameters loaded../model/94la.pth\n",
      "(Dropout: 0.5) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0333\n",
      "123.0474208772111\n",
      "178.59494687583882\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0041\n",
      "19.883415370508693\n",
      "29.195810815494983\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0031\n",
      "15.706283001102003\n",
      "23.11137501237029\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0030\n",
      "13.62807230619506\n",
      "20.12608383083255\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0029\n",
      "12.451894196040454\n",
      "18.412932197162842\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0025\n",
      "12.041727637256669\n",
      "17.80224903010076\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0020\n",
      "11.883573573026261\n",
      "17.581384205160912\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0018\n",
      "11.255451763211434\n",
      "16.706003091240557\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0018\n",
      "10.140675970160956\n",
      "15.149540440196684\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0019\n",
      "9.469691020818845\n",
      "14.234300405670094\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0019\n",
      "9.043134940064608\n",
      "13.64723692212897\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0019\n",
      "8.66657871967571\n",
      "13.125092048440782\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0019\n",
      "8.35907085896064\n",
      "12.707634713928256\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0019\n",
      "8.131178575182105\n",
      "12.403300035698754\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0017\n",
      "7.917886215598861\n",
      "12.121273968053933\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0016\n",
      "7.711835147507416\n",
      "11.849411172528786\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0013\n",
      "7.5320190355146455\n",
      "11.604276742600828\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0011\n",
      "7.363842397420817\n",
      "11.378234744271309\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0010\n",
      "7.237155926202384\n",
      "11.199068550064176\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0009\n",
      "7.078919136276226\n",
      "10.985822603534903\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0008\n",
      "6.945745972906974\n",
      "10.82176246607146\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0007\n",
      "6.844969324532649\n",
      "10.698839487161386\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0006\n",
      "6.751587889282831\n",
      "10.591350009078912\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0006\n",
      "6.650531297696389\n",
      "10.456963030119784\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0006\n",
      "6.572651205756417\n",
      "10.364030049141284\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0006\n",
      "6.485600766045373\n",
      "10.252146999514398\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0006\n",
      "6.415369620434683\n",
      "10.161391308587687\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0006\n",
      "6.351459521983434\n",
      "10.080930336077994\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0005\n",
      "6.317119208605233\n",
      "10.038813675819448\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0005\n",
      "6.239576021264182\n",
      "9.948513502926849\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0005\n",
      "6.256199053152427\n",
      "9.951116975957644\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0005\n",
      "6.228149923237583\n",
      "9.913976084269649\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0004\n",
      "6.1232706889659845\n",
      "9.767418489257254\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0004\n",
      "6.074641931465322\n",
      "9.695277326917559\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0004\n",
      "6.0457140325950105\n",
      "9.657856701740679\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0004\n",
      "6.055965201760772\n",
      "9.65453613334592\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [37/300], Step [437/437], Loss: 0.0004\n",
      "5.977051288524232\n",
      "9.56120126404627\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [38/300], Step [437/437], Loss: 0.0004\n",
      "5.954319302882501\n",
      "9.533341800857016\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [39/300], Step [437/437], Loss: 0.0004\n",
      "5.93792112667589\n",
      "9.500867409120703\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [40/300], Step [437/437], Loss: 0.0004\n",
      "5.9470045352522485\n",
      "9.514793319584726\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [41/300], Step [437/437], Loss: 0.0003\n",
      "5.924125222286763\n",
      "9.481563594267891\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [42/300], Step [437/437], Loss: 0.0004\n",
      "5.983476400962402\n",
      "9.554561662203135\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [43/300], Step [437/437], Loss: 0.0003\n",
      "5.912051459108792\n",
      "9.463000978085436\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [44/300], Step [437/437], Loss: 0.0003\n",
      "5.95835660669375\n",
      "9.519854102077424\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [45/300], Step [437/437], Loss: 0.0003\n",
      "5.956088720834591\n",
      "9.51123659793075\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [46/300], Step [437/437], Loss: 0.0003\n",
      "5.946734018853832\n",
      "9.486824907193284\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [47/300], Step [437/437], Loss: 0.0003\n",
      "5.988794154517247\n",
      "9.53923385253432\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:46 batch_size64 lr:0.0001 kernel_size3 stride:1 train_acc:5.912051459108792 test_acc:9.463000978085436\n",
      "Best model parameters loaded: ./model/95la.pth\n",
      "Best model parameters loaded../model/95la.pth\n",
      "(Dropout: 0.5) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0444\n",
      "120.65313231458978\n",
      "175.0005386928973\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0097\n",
      "21.47678728293313\n",
      "31.47861979855623\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0072\n",
      "16.377193374344863\n",
      "24.076792898473133\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0060\n",
      "14.220363173401042\n",
      "20.9860454548397\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0046\n",
      "13.106887896992257\n",
      "19.36820648075946\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0034\n",
      "12.421872710162848\n",
      "18.3886749201077\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0029\n",
      "11.815664289644687\n",
      "17.537769085367977\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0025\n",
      "11.329968164153502\n",
      "16.850656159247976\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0020\n",
      "10.963904024148876\n",
      "16.325484085466503\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0018\n",
      "10.568604152528016\n",
      "15.76714130436858\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0016\n",
      "10.235933941925012\n",
      "15.289707970170246\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0014\n",
      "9.881132444144084\n",
      "14.78277650352327\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0013\n",
      "9.518324299369004\n",
      "14.264693239921971\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0012\n",
      "9.242546123856032\n",
      "13.876675515673066\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0011\n",
      "8.975199989052586\n",
      "13.511337663317848\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0010\n",
      "8.784095307223057\n",
      "13.25515442679746\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0009\n",
      "8.613986391417317\n",
      "13.020884673542476\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0009\n",
      "8.46883346856106\n",
      "12.824833437260773\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0009\n",
      "8.36656822499793\n",
      "12.678324459989648\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0008\n",
      "8.247490443759462\n",
      "12.515491140506969\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0008\n",
      "8.127584130457723\n",
      "12.353399495067656\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0007\n",
      "8.066753542628879\n",
      "12.262577541860535\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0007\n",
      "8.015239380371419\n",
      "12.186724647261912\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0007\n",
      "7.990649202364254\n",
      "12.145974439622522\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0007\n",
      "7.940205875159985\n",
      "12.075771916693006\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0006\n",
      "7.937201431098674\n",
      "12.066334458533449\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0006\n",
      "7.894396739904527\n",
      "12.007049954981465\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0006\n",
      "7.908363741173107\n",
      "12.032807370938166\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0006\n",
      "7.9018824549228635\n",
      "12.031006536046235\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0006\n",
      "7.894150706601886\n",
      "12.02455347521044\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0006\n",
      "7.935434795742435\n",
      "12.085416275060314\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:30 batch_size64 lr:0.0001 kernel_size5 stride:1 train_acc:7.894396739904527 test_acc:12.007049954981465\n",
      "Best model parameters loaded: ./model/96la.pth\n",
      "Best model parameters loaded../model/96la.pth\n",
      "(Dropout: 0.5) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0265\n",
      "85.9240909380733\n",
      "124.6159825102053\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0055\n",
      "19.309150453627847\n",
      "28.588655884479685\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0042\n",
      "14.638355193908252\n",
      "21.852895580615108\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0039\n",
      "12.385349708998183\n",
      "18.571714884179897\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0028\n",
      "11.417346499167108\n",
      "17.16249507311046\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0019\n",
      "10.899301857879763\n",
      "16.34641045556495\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0013\n",
      "10.368558752634733\n",
      "15.570133165520923\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0010\n",
      "9.642026058555782\n",
      "14.516728263576365\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0008\n",
      "9.00480104621155\n",
      "13.60470409384824\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0006\n",
      "8.648468867439421\n",
      "13.102484268132056\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0005\n",
      "8.289798102646072\n",
      "12.601603129253261\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0004\n",
      "7.967332367523807\n",
      "12.159888760194976\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0004\n",
      "7.602040574901021\n",
      "11.646780491254729\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0003\n",
      "7.295492180196643\n",
      "11.226790658166355\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0003\n",
      "7.055331118411205\n",
      "10.909663193414353\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0003\n",
      "6.827857818856617\n",
      "10.6056781742131\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0002\n",
      "6.566492101702545\n",
      "10.254257733060385\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0002\n",
      "6.406156204361879\n",
      "10.04880654809598\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0002\n",
      "6.20833884604902\n",
      "9.794171137368298\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0002\n",
      "6.092947192996921\n",
      "9.65011582547028\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0002\n",
      "5.929950918357066\n",
      "9.454823055411497\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0002\n",
      "5.792998471180571\n",
      "9.280768236537314\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0002\n",
      "5.842924660999883\n",
      "9.357439096636542\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0002\n",
      "5.732324180043558\n",
      "9.220418995993603\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0002\n",
      "5.588637799515319\n",
      "9.038593972163708\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0001\n",
      "5.5580715714502675\n",
      "9.01814245146842\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0001\n",
      "5.599891937375955\n",
      "9.079474993035005\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0001\n",
      "5.3817406973164665\n",
      "8.793150887194113\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0001\n",
      "5.371522943700037\n",
      "8.799142379000088\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0001\n",
      "5.349586909933026\n",
      "8.779354564050857\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0001\n",
      "5.269414942915267\n",
      "8.684737302257744\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0001\n",
      "5.151720544843226\n",
      "8.534509109760739\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0001\n",
      "5.170346643638528\n",
      "8.559533656217774\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0001\n",
      "5.142314770563976\n",
      "8.52398279598141\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0001\n",
      "5.147878237023106\n",
      "8.533410183200429\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0001\n",
      "5.0519023939498275\n",
      "8.40507764282016\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [37/300], Step [437/437], Loss: 0.0001\n",
      "5.028056101341771\n",
      "8.384854972069878\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [38/300], Step [437/437], Loss: 0.0001\n",
      "5.0923386317628445\n",
      "8.477298508808028\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [39/300], Step [437/437], Loss: 0.0001\n",
      "4.981593968160357\n",
      "8.331901183965114\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [40/300], Step [437/437], Loss: 0.0001\n",
      "5.0022497626140225\n",
      "8.35482693253559\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [41/300], Step [437/437], Loss: 0.0001\n",
      "4.925308756611568\n",
      "8.249421162577677\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [42/300], Step [437/437], Loss: 0.0001\n",
      "4.851122004326069\n",
      "8.151649201260986\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [43/300], Step [437/437], Loss: 0.0001\n",
      "4.924233673926714\n",
      "8.260441456799901\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [44/300], Step [437/437], Loss: 0.0001\n",
      "4.849363548813142\n",
      "8.157038439349732\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [45/300], Step [437/437], Loss: 0.0001\n",
      "4.874441308679687\n",
      "8.188649767396498\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [46/300], Step [437/437], Loss: 0.0001\n",
      "4.820127436121909\n",
      "8.122007001576861\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [47/300], Step [437/437], Loss: 0.0001\n",
      "4.9124016606334235\n",
      "8.236382937405777\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [48/300], Step [437/437], Loss: 0.0001\n",
      "4.856176920568666\n",
      "8.165182499417147\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [49/300], Step [437/437], Loss: 0.0000\n",
      "4.8593125330202955\n",
      "8.166622016313186\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [50/300], Step [437/437], Loss: 0.0001\n",
      "4.786997721749322\n",
      "8.068533084206141\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [51/300], Step [437/437], Loss: 0.0000\n",
      "4.745148369684076\n",
      "8.02380092457077\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [52/300], Step [437/437], Loss: 0.0001\n",
      "5.173596648988772\n",
      "8.57585041152151\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [53/300], Step [437/437], Loss: 0.0001\n",
      "5.525329934821835\n",
      "9.016819910889916\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [54/300], Step [437/437], Loss: 0.0000\n",
      "5.392913624007426\n",
      "8.851378188662842\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [55/300], Step [437/437], Loss: 0.0000\n",
      "5.206881542515986\n",
      "8.625570544095446\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:54 batch_size64 lr:0.0001 kernel_size5 stride:1 train_acc:4.745148369684076 test_acc:8.02380092457077\n",
      "Best model parameters loaded: ./model/97la.pth\n",
      "Best model parameters loaded../model/97la.pth\n",
      "(Dropout: 0.5) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0488\n",
      "53.998274674957464\n",
      "78.41762403976857\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0082\n",
      "17.37881319667896\n",
      "25.800833999138668\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0044\n",
      "13.105075691168633\n",
      "19.543749905575744\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0025\n",
      "11.958057231960685\n",
      "17.91562578919254\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0018\n",
      "11.33152325285674\n",
      "17.024957721670567\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0014\n",
      "10.452269467071385\n",
      "15.77798459310192\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0013\n",
      "9.506277175253523\n",
      "14.434159646132496\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0012\n",
      "8.936683378793612\n",
      "13.602331246836137\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0011\n",
      "8.493653935795473\n",
      "12.955458493716636\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0012\n",
      "8.147580268931884\n",
      "12.451539178719363\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0012\n",
      "7.935943139884989\n",
      "12.149741593154555\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0011\n",
      "7.801915032615299\n",
      "11.964172264970506\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0010\n",
      "7.801929718704377\n",
      "11.954370527443178\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0009\n",
      "7.670145766609511\n",
      "11.765425625490622\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0008\n",
      "7.60433975273531\n",
      "11.672770662751295\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0008\n",
      "7.442565905455206\n",
      "11.444665481210814\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0008\n",
      "7.260142066544411\n",
      "11.186619177039294\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0008\n",
      "7.089093348171472\n",
      "10.94424803769087\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0009\n",
      "6.992401336820859\n",
      "10.797466692945575\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0008\n",
      "6.93252119990686\n",
      "10.704336252647318\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0008\n",
      "6.889076293014881\n",
      "10.642113837797087\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0007\n",
      "6.867234309838441\n",
      "10.610490141493585\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0008\n",
      "6.8171201138503745\n",
      "10.535239838792977\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0007\n",
      "6.808177801607592\n",
      "10.52647099994738\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0007\n",
      "6.782498705037684\n",
      "10.476307760331313\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0008\n",
      "6.78349474746655\n",
      "10.456867617991447\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0007\n",
      "6.764216619798092\n",
      "10.432462310346914\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0007\n",
      "6.748003087895989\n",
      "10.407795744675212\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0007\n",
      "6.723225754714797\n",
      "10.377323568659827\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0007\n",
      "6.755664534199892\n",
      "10.412549632019529\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0007\n",
      "6.687888568729801\n",
      "10.326055998992919\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [32/300], Step [437/437], Loss: 0.0007\n",
      "6.693936080722466\n",
      "10.315319448720537\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [33/300], Step [437/437], Loss: 0.0007\n",
      "6.724438494967584\n",
      "10.351788785488155\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [34/300], Step [437/437], Loss: 0.0007\n",
      "6.724163296623226\n",
      "10.350510950651588\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [35/300], Step [437/437], Loss: 0.0006\n",
      "6.729370854112846\n",
      "10.356758157743293\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [36/300], Step [437/437], Loss: 0.0006\n",
      "6.705991488855182\n",
      "10.328329629036357\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:35 batch_size64 lr:0.0001 kernel_size7 stride:1 train_acc:6.693936080722466 test_acc:10.315319448720537\n",
      "Best model parameters loaded: ./model/98la.pth\n",
      "Best model parameters loaded../model/98la.pth\n",
      "(Dropout: 0.5) (kernel: [3, 5, 7]) (padding: 0) (lr:0.0001)\n",
      "Epoch [1/300], Step [437/437], Loss: 0.0519\n",
      "89.02906545605327\n",
      "129.70002515401956\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [2/300], Step [437/437], Loss: 0.0062\n",
      "18.275424919015354\n",
      "27.029027241669493\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [3/300], Step [437/437], Loss: 0.0050\n",
      "14.091867053465494\n",
      "20.937392305407087\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [4/300], Step [437/437], Loss: 0.0034\n",
      "12.14686160961098\n",
      "18.13150025262431\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [5/300], Step [437/437], Loss: 0.0025\n",
      "10.953801187228386\n",
      "16.444431291292844\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [6/300], Step [437/437], Loss: 0.0020\n",
      "10.119705541581878\n",
      "15.284921295481642\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [7/300], Step [437/437], Loss: 0.0015\n",
      "9.652703108628936\n",
      "14.662216754464634\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [8/300], Step [437/437], Loss: 0.0013\n",
      "8.738947719202754\n",
      "13.414691340217287\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [9/300], Step [437/437], Loss: 0.0015\n",
      "8.255784016473234\n",
      "12.75532975627452\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [10/300], Step [437/437], Loss: 0.0013\n",
      "7.827170630177695\n",
      "12.162908227005051\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [11/300], Step [437/437], Loss: 0.0010\n",
      "7.537679496375603\n",
      "11.77648552035904\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [12/300], Step [437/437], Loss: 0.0009\n",
      "7.193820200605672\n",
      "11.29960136677539\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [13/300], Step [437/437], Loss: 0.0008\n",
      "6.8917254910849275\n",
      "10.868558247176479\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [14/300], Step [437/437], Loss: 0.0006\n",
      "6.715885590606507\n",
      "10.606124067106203\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [15/300], Step [437/437], Loss: 0.0005\n",
      "6.4238292795549\n",
      "10.185949299276682\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [16/300], Step [437/437], Loss: 0.0004\n",
      "6.234476775335836\n",
      "9.918716650659501\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [17/300], Step [437/437], Loss: 0.0003\n",
      "6.1331582912660165\n",
      "9.767000890449292\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [18/300], Step [437/437], Loss: 0.0003\n",
      "5.8485692012807835\n",
      "9.368383487731144\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [19/300], Step [437/437], Loss: 0.0002\n",
      "5.769923273494433\n",
      "9.265243408016497\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [20/300], Step [437/437], Loss: 0.0002\n",
      "5.582532064192325\n",
      "9.004912362874128\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [21/300], Step [437/437], Loss: 0.0002\n",
      "5.441421709871119\n",
      "8.814483619396228\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [22/300], Step [437/437], Loss: 0.0001\n",
      "5.387137992367305\n",
      "8.758864869267871\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [23/300], Step [437/437], Loss: 0.0001\n",
      "5.27654986402525\n",
      "8.605157251522762\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [24/300], Step [437/437], Loss: 0.0001\n",
      "5.167112481640941\n",
      "8.474941178973772\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [25/300], Step [437/437], Loss: 0.0001\n",
      "5.063041672643738\n",
      "8.334824218454637\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [26/300], Step [437/437], Loss: 0.0001\n",
      "5.018961286671183\n",
      "8.3149963641957\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [27/300], Step [437/437], Loss: 0.0001\n",
      "4.943721657031477\n",
      "8.226197861918417\n",
      "→ Model improved. Saving...\n",
      "------------------------------\n",
      "Epoch [28/300], Step [437/437], Loss: 0.0001\n",
      "4.9292256063755815\n",
      "8.250160346944272\n",
      "→ No improvement. Patience: 1/4\n",
      "------------------------------\n",
      "Epoch [29/300], Step [437/437], Loss: 0.0001\n",
      "4.904867948816019\n",
      "8.237888282232745\n",
      "→ No improvement. Patience: 2/4\n",
      "------------------------------\n",
      "Epoch [30/300], Step [437/437], Loss: 0.0001\n",
      "4.9041379236516995\n",
      "8.260005234304343\n",
      "→ No improvement. Patience: 3/4\n",
      "------------------------------\n",
      "Epoch [31/300], Step [437/437], Loss: 0.0000\n",
      "5.06426938686882\n",
      "8.501555042258678\n",
      "→ No improvement. Patience: 4/4\n",
      "Early stopping triggered.\n",
      "parameter sava...epoch:30 batch_size64 lr:0.0001 kernel_size7 stride:1 train_acc:4.943721657031477 test_acc:8.226197861918417\n",
      "Best model parameters loaded: ./model/99la.pth\n",
      "Best model parameters loaded../model/99la.pth\n"
     ]
    }
   ],
   "source": [
    "# Latitude Regressor\n",
    "dropout = [0.3,0.4,0.5]\n",
    "lrs = [0.0001]\n",
    "kernel_sizes = [3,5,7]\n",
    "strides = [1]\n",
    "model_order = 82\n",
    "weight_decays = [0.001, 0.0001]\n",
    "for val in dropout:\n",
    "    for lr in lrs:\n",
    "        for kernel_size in kernel_sizes:\n",
    "            for stride in strides:\n",
    "                for weight_decay in weight_decays:\n",
    "                    print(f'(Dropout: {val}) (kernel: {kernel_sizes}) (padding: {0}) (lr:{lr})')\n",
    "                    padding = 0\n",
    "                    cnn = CNNRegressor(n_targets=1, dropout=val, kernel_size=kernel_size, padding=padding) # Longitude + Latitude are targets\n",
    "                    cnn.fit(X_train_la_cnn, y_train_la_cnn, X_test_la_cnn, y_test_la_cnn)\n",
    "                    # cnn.fit(X_trainCo_cnn, y_trainCo_cnn_scaled, X_testCo_cnn, y_testCo_cnn_scaled)\n",
    "                    # cnn.train(num_epochs=100,eval_train=True, min_max_dist=min_max_dist, lr=0.0001, batch_size=16)\n",
    "                    cnn.train_model(num_epochs=300,eval_train=True, min_max_dist=min_max_dist, lr=lr, batch_size=64, patience = 4, sava_model_name=\"./model/\"+str(model_order)+\"la.pth\", kernel_size = kernel_size, stride = stride, weight_decay=weight_decay, dropout= val)\n",
    "                    model_order += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T19:24:43.241356Z",
     "start_time": "2025-08-22T12:51:45.786716Z"
    }
   },
   "id": "113d13cee2747406",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set average Euclidean distance: 11.43 meters\n",
      "Test set average Euclidean distance: 8.38 meters\n",
      "Test set average Euclidean distance: 9.56 meters\n",
      "Test set average Euclidean distance: 7.37 meters\n",
      "Test set average Euclidean distance: 9.60 meters\n",
      "Test set average Euclidean distance: 6.19 meters\n",
      "Test set average Euclidean distance: 11.57 meters\n",
      "Test set average Euclidean distance: 9.46 meters\n",
      "Test set average Euclidean distance: 9.55 meters\n",
      "Test set average Euclidean distance: 7.24 meters\n",
      "Test set average Euclidean distance: 9.50 meters\n",
      "Test set average Euclidean distance: 6.48 meters\n",
      "Test set average Euclidean distance: 10.68 meters\n",
      "Test set average Euclidean distance: 7.71 meters\n",
      "Test set average Euclidean distance: 9.50 meters\n",
      "Test set average Euclidean distance: 7.04 meters\n",
      "Test set average Euclidean distance: 9.75 meters\n",
      "Test set average Euclidean distance: 6.33 meters\n"
     ]
    }
   ],
   "source": [
    "from Algorithms.CNN import CNNRegressor\n",
    "import numpy as np\n",
    "dropout = [0.3,0.4,0.5]\n",
    "lrs = [0.0001]\n",
    "kernel_sizes = [3,5,7]\n",
    "strides = [1]\n",
    "model_order = 82\n",
    "weight_decays = [0.001, 0.0001]\n",
    "for val in dropout:\n",
    "    for lr in lrs:\n",
    "        for kernel_size in kernel_sizes:\n",
    "            for stride in strides:\n",
    "                for weight_decay in weight_decays:\n",
    "                    # 假设你的模型是 CNNRegressor，有两个目标值\n",
    "                    model_la = CNNRegressor(n_targets=1, kernel_size=kernel_size, stride=stride, padding=0, dropout=val)\n",
    "                     \n",
    "                    # 加载保存的权重\n",
    "                    model_la.load_model(\"./model/\"+str(model_order)+\"la.pth\")\n",
    "                    model_la.X_train = np.zeros_like(X_valid_la_cnn)  # 随便补一个假的训练集占位，不会用到\n",
    "                    model_la.Y_train = np.zeros_like(y_valid_la_cnn)\n",
    "                    # 注意：还需要加载相应的数据\n",
    "                    model_la.X_test = X_valid_la_cnn\n",
    "                    model_la.Y_test = y_valid_la_cnn\n",
    "                    # model.X_train = None\n",
    "                    # model.y_train = None\n",
    "                    model_la.create_loaders(batch_size=64)\n",
    "                    \n",
    "                    \n",
    "                    predictions_la, targets_la = model_la.test(model_la.loaders['test'])\n",
    "                    \n",
    "                    # 若你是多目标回归任务（例如RSSI定位），建议继续后处理：\n",
    "                    preds_denorm_la = denormalize_coords(predictions_la, min_max_dist[0], min_max_dist[1])\n",
    "                    targets_denorm_la = denormalize_coords(targets_la, min_max_dist[0], min_max_dist[1])\n",
    "                    \n",
    "                    average_dist_la = mean_euclidean_distance(preds_denorm_la, targets_denorm_la)\n",
    "                    # print(f\"Test set average Euclidean distance: {average_dist_la:.2f} meters\")\n",
    "                    \n",
    "                    model_lo = CNNRegressor(n_targets=1, kernel_size=kernel_size, stride=stride, padding=0, dropout=val)\n",
    "    \n",
    "                    # 加载保存的权重\n",
    "                    model_lo.load_model(\"./model/\"+str(model_order)+\"lo.pth\")\n",
    "                    model_lo.X_train = np.zeros_like(X_valid_lo_cnn)  # 随便补一个假的训练集占位，不会用到\n",
    "                    model_lo.Y_train = np.zeros_like(y_valid_lo_cnn)\n",
    "                    # 注意：还需要加载相应的数据\n",
    "                    model_lo.X_test = X_valid_lo_cnn\n",
    "                    model_lo.Y_test = y_valid_lo_cnn\n",
    "                    # model.X_train = None\n",
    "                    # model.y_train = None\n",
    "                    model_lo.create_loaders(batch_size=64)\n",
    "                    \n",
    "                    \n",
    "                    predictions_lo, targets_lo = model_lo.test(model_lo.loaders['test'])\n",
    "                    \n",
    "                    # 若你是多目标回归任务（例如RSSI定位），建议继续后处理：\n",
    "                    preds_denorm_lo = denormalize_coords(predictions_lo, min_max_dist[2], min_max_dist[3])\n",
    "                    targets_denorm_lo = denormalize_coords(targets_lo, min_max_dist[2], min_max_dist[3])\n",
    "                    \n",
    "                    average_dist_lo = mean_euclidean_distance(preds_denorm_lo, targets_denorm_lo)\n",
    "                    # print(f\"Test set average Euclidean distance: {average_dist_lo:.2f} meters\")\n",
    "                    \n",
    "                    preds_denorm = np.hstack([preds_denorm_la, preds_denorm_lo]) \n",
    "                    targets_denorm = np.hstack([targets_denorm_la, targets_denorm_lo])\n",
    "                    \n",
    "                    average_dist = mean_euclidean_distance(preds_denorm, targets_denorm)\n",
    "                    print(f\"Test set average Euclidean distance: {average_dist:.2f} meters\")\n",
    "                    model_order+=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-24T06:25:53.255852Z",
     "start_time": "2025-08-24T06:23:31.612372Z"
    }
   },
   "id": "8767898c64ba6c20",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from Algorithms.CNN import CNNRegressor\n",
    "import numpy as np\n",
    "\n",
    "# 假设你的模型是 CNNRegressor，有两个目标值\n",
    "model_la = CNNRegressor(n_targets=1, kernel_size=7, stride=1, padding=3, dropout=0.2)\n",
    " \n",
    "# 加载保存的权重\n",
    "model_la.load_model('best_model_la.pth')\n",
    "model_la.X_train = np.zeros_like(X_valid_la_cnn)  # 随便补一个假的训练集占位，不会用到\n",
    "model_la.Y_train = np.zeros_like(y_valid_la_cnn)\n",
    "# 注意：还需要加载相应的数据\n",
    "model_la.X_test = X_valid_la_cnn\n",
    "model_la.Y_test = y_valid_la_cnn\n",
    "# model.X_train = None\n",
    "# model.y_train = None\n",
    "model_la.create_loaders(batch_size=64)\n",
    "\n",
    "\n",
    "predictions_la, targets_la = model_la.test(model_la.loaders['test'])\n",
    "\n",
    "# 若你是多目标回归任务（例如RSSI定位），建议继续后处理：\n",
    "preds_denorm_la = denormalize_coords(predictions_la, min_max_dist[0], min_max_dist[1])\n",
    "targets_denorm_la = denormalize_coords(targets_la, min_max_dist[0], min_max_dist[1])\n",
    "\n",
    "average_dist_la = mean_euclidean_distance(preds_denorm_la, targets_denorm_la)\n",
    "print(f\"Test set average Euclidean distance: {average_dist_la:.2f} meters\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T19:25:20.496205Z",
     "start_time": "2025-08-23T19:25:20.496205Z"
    }
   },
   "id": "ead1f55ccc82f262",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from Algorithms.CNN import CNNRegressor\n",
    "import numpy as np\n",
    "# 假设你的模型是 CNNRegressor，有两个目标值\n",
    "model_lo = CNNRegressor(n_targets=1, kernel_size=7, stride=1, padding=3, dropout=0.2)\n",
    " \n",
    "# 加载保存的权重\n",
    "model_lo.load_model('best_model_lo')\n",
    "model_lo.X_train = np.zeros_like(X_valid_lo_cnn)  # 随便补一个假的训练集占位，不会用到\n",
    "model_lo.Y_train = np.zeros_like(y_valid_lo_cnn)\n",
    "# 注意：还需要加载相应的数据\n",
    "model_lo.X_test = X_valid_lo_cnn\n",
    "model_lo.Y_test = y_valid_lo_cnn\n",
    "# model.X_train = None\n",
    "# model.y_train = None\n",
    "model_lo.create_loaders(batch_size=64)\n",
    "\n",
    "\n",
    "predictions_lo, targets_lo = model_lo.test(model_lo.loaders['test'])\n",
    "\n",
    "# 若你是多目标回归任务（例如RSSI定位），建议继续后处理：\n",
    "preds_denorm_lo = denormalize_coords(predictions_lo, min_max_dist[2], min_max_dist[3])\n",
    "targets_denorm_lo = denormalize_coords(targets_lo, min_max_dist[2], min_max_dist[3])\n",
    "\n",
    "average_dist_lo = mean_euclidean_distance(preds_denorm_lo, targets_denorm_lo)\n",
    "print(f\"Test set average Euclidean distance: {average_dist_lo:.2f} meters\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T19:25:20.497206Z",
     "start_time": "2025-08-23T19:25:20.497206Z"
    }
   },
   "id": "e575b706fb3a4bd5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "preds_denorm = np.hstack([preds_denorm_la, preds_denorm_lo]) \n",
    "targets_denorm = np.hstack([targets_denorm_la, targets_denorm_lo])\n",
    "\n",
    "average_dist = mean_euclidean_distance(preds_denorm, targets_denorm)\n",
    "print(f\"Test set average Euclidean distance: {average_dist:.2f} meters\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-08-23T19:25:20.498212Z"
    }
   },
   "id": "5c839fbe57e68073",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-08-23T19:25:20.499213Z"
    }
   },
   "id": "6f54d5749cf02f3c",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
