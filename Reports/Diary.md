# Project Diary  
### Machine Learning Based Indoor Positioning    
**19/06/2024**  
I had a meeting with my supervisor to discuss the outline of my project 'Machine Learning Based Indoor Positioning'. I now understand how to approach this project and will produce a project plan detailing the deliverables and objectives of the project. Before this, I completed the recommended reading and looked at the competition dataset (UJIIndoorLoc) that will be used in the project.  
**21/06/2024**  
I have completed the project plan, and will now start working on the project following the specified timeline of the plan. I will begin analysing the UJIIndoorLoc dataset and produce a report on my findings.  
**24/06/2024**  
I have started analysing the UJIIndoorLoc dataset; I have begun by looking at the information detailed in the dataset archive. I now have a better understanding of the general use of the database.   
**25/06/2024**  
I received feedback on my project plan that I need to implement; I need to add a more detailed overview from week seven onwards, which will better describe my approach to the dissertation. Once I complete this, I can return to working on the dataset analysis. I have added code to the notebook that allows you to retrieve the dataset regardless of the operating system someone is working from. I will start the thorough analysis of the dataset now.  
**26/06/2024**  
I have begun looking at the UJIIndoorLoc dataset, primarily looking at the different attributes and the data types of these attributes and their ranges. The dataset comprises 520 WAP readings, and other relevant information like longitude/latitude and position identifiers (BuildingID, Floor, SpaceID) I better understand the make-up of the dataset and how I could use this data for my project. I have started to build plots with the dataset using Matplotlib, but the plot came out wrong I will try and work on this further tomorrow.  
**27/06/2024**  
Today I began making the plots for the dataset again, I managed to fix the problem I had yesterday with making the plots. I was using plt.plot() instead of plt.scatter(), I have produced a scatter plot of one of the buildings in the dataset using longitude and latitude as the axes and colour-coded the points by the floor ID. I also made the same plot but as a 3D projection, however, it doesn't look as nice so I'm not sure if I will keep it. I continued to make some more plots, I made a scatter plot for each building and one of all the buildings together. I have also made a plot of the Buildings using the Relative Position attribute in the dataset (the relative position is one if we are inside the space and two if we are outside the space), this plot doesn't convey a meaningful representation of the dataset so I might delete these later.  
**01/07/2024**  
I have made more plots for the dataset analysis notebook, and I have made bar charts to show the number of samples for each building in the dataset. I successfully created a histogram plot showing how many active WAPs there are for each row of data. I wanted to make a chart showing the WAP activity for the 520 WAPs but it hasn't come out correctly so I will work on this later.  
**02/07/2024**  
I have added histograms that split the data by their RSSI Wifi strength. I was also able to fix the plot for the individual WAP activity, I now have a plot that shows the WAP activity for the 520 WAPs in the dataset. I decided to make bar charts showing the number of samples taken on each floor for each building. I also started looking at the validation data which I forgot to do previously.  
**03/07/2024**  
I spent today making the existing plots look better by adding axis labels and adding line edges to the plots. I have created a bar chart looking at both training data and validation data together for the number of samples per building. I produced a similar bar chart for the number of samples for each floor per building. The notebook now looks much cleaner.  
**04/07/2024**  
Today I recreated the plots that I have already made but this time with the validation data. I have also made histograms looking at the mean RSSI over all of the samples. I also cleaned up some of the testing code in the notebook.   
**09/07/2024**  
I have created another plot regarding the USERID attribute, I have made a bar chart looking at the frequency of each USERID in the dataset. Interestingly all of the validation data have the USERID 0 (this is included in the plot).  
**11/07/2024**  
Made another plot looking at the distribution of the PhoneID attribute, the plot shows a bar chart looking at the frequency of each PhoneID over the dataset (I did this for both training and validation sets). Each PhoneID is a different phone, so I thought it would be interesting from a data collection perspective (you would want the wifi localization algorithm to work for any device). I started work on the exploratory data analysis and have made a PCA plot describing the first two principal components.  
**12/07/2024**  
I looked at using TSNE to describe the dataset, TSNE is another exploratory data analysis technique. I was able to produce a t-SNE visualization and experimented with different levels of perplexity. TSNE is time-consuming so I used PCA to reduce the components to 50 to make it easier to produce. For the PCA I created plots looking at the variance explained by each PC and the cumulative proportion of variance explained by each principal component.  
**13/07/2024**  
I went to work back on the TSNE plot and realised I had made a mistake, the parameter 'init' was set to 'random' and not 'pca' which was affecting the plot. I also looked at producing a UMAP plot of the dataset, UMAP is another exploratory data analysis technique. UMAP is better as it can produce a plot of all the features (no need for feature reduction like in TSNE). The UMAP plot shows a similar picture to TSNE but with more overlap between the data clusters.   
**15/07/2024**  
I decide to make another PCA plot but this time omitting the WAP data. This showed a clear separation of clusters for each building. I also decided to clean up the notebook of testing code. I still need to add comments to each plot describing what they convey.  
**18/07/2024**  
Today I started producing algorithms from scratch, I began making the KNN class that I will later use on the UJIIndoorLoc dataset. I started by creating the skeleton for the KNN algorithm. I have added code to most of the methods (constructor, fit, get_distance). I will finish the other methods later.  
**19/07/2024**  
I have finished making the KNN class and have documented the class fully. I will test this class on a synthetic dataset to see if it works properly.  
**22/07/2024**  
I have started working on making a Decision Tree algorithm, I have completed a tutorial that makes a Decision Tree from scratch. This tutorial has shown me all the functionality I require to make my own Decision Tree class.  
**23/07/2024**  
I have extended the tutorial code to work in the case of regression, this will be useful for my implementation. I have started working on my DecisionTree class, the class contains a superclass (DecisionTree) and two child classes (DecisionTreeClassifier and DecisionTreeRegressor). I have implemented some of the functionality for these classes, but there is more to do.  
**24/07/2024**   
I have completed my DecisionTree implementation, and all of the necessary functionality has been added to the class/subclasses. I will test the code on a synthetic dataset to check if the algorithm works as intended.  
**25/07/2024**  
Today I worked on creating my LinearRegression implementation. I have implemented the code with the help of a lab sheet from the CS5100 Data Analysis module. The implementation minimizes the Least Squares Objective using the data, finding the global minimum and best fit for the regression model. There is a variation that uses gradient descent to find the global minimum but I haven't included this in my implementation. I will test this algorithm on a synthetic dataset to check if it works as intended.  
**26/07/2024**  
I found a tutorial detailing how to make a Support Vector Machine from scratch, so I have spent most of today following the tutorial. The tutorial shows how to make both binary SVMs and multiclass SVMs.  
**27/07/2024**  
I have finished working on the SVM tutorial, and have begun implementing my own SVM class using the ideas from the tutorial. I have tested my implementation in the tutorial jupyter notebook. I will further test my SVM implementation with a synthetic dataset to see if it works as intended.  
**28/07/2024**  
The final algorithm I wanted to implement was a Convolutional Neural Network, I first looked at existing CNN implementations and found a tutorial on the topic. I began working on my own CNN implementation after completing the CNN tutorial. The tutorial involved creating a CNN model for the MNIST dataset. My CNN implementation uses the Pytorch library, which simplifies most of the code implementation. However, an error appeared when using Dataloaders which I will probably resolve tomorrow.  
**29/07/2024**  
I have figured out why the Dataloaders class was breaking the code, and corrected my implementation. I then finished adding all the remaining functionality to the class and fully documented each method. My CNN implementation uses a super class (CNN) and two child classes (CNNClassifier and CNNRegressor), so the model is compatible with classification and regression problems. I will test my implementation to see if works as intended.  
**01/08/2024**  
I have spent the previous days adding comments to the plots describing the UJIIndooorLoc dataset. Today, I will start working on testing the machine learning algorithms I made on a synthetic dataset. I have begun work on testing my KNN implementation; I will probably complete the rest tomorrow.  
**02/08/2024**  
I have tested my KNN implementation, and it gave similar results to the scikit-learn implementation. I will start testing the other algorithms on the generated synthetic data to evaluate their performance. I have just finished testing my Linear Regression implementation, and it is working as well as the scikit-learn implementation.  
**03/08/2024**  
Today, I worked on testing my DecisionTree implementations; my decision tree classifier worked as intended, and it showed adequate performance comparable to its scikit-learn equivalent. The decision tree regressor, on the other hand, was producing very uneven splits; it kept splitting the data by only one data item. Its performance was poor, so I spent the entire day figuring out why it was doing this. Eventually, I discovered it was due to using mean squared error to determine the best split. The function favoured splits with a single data item as it reduced the MSE the most. Therefore, I changed the function to calculate the squared error instead so the score wouldn't be heavily reduced by having a larger split bin. Testing the updated DecisionTreeRegressor class showed excellent performance on par with the scikit 
implementation.  
**04/08/2024**  
I have been testing the SVM class in a jupyter notebook, and from the testing, I found out that the gamma parameter used in the 'rbf' kernel was fixed as a constant. Therefore, I made it possible to change the gamma hyperparameter to achieve better performance. Testing the SVM class against the scikit-learn equivalent showed similar accuracy. I have started testing the CNN class to check if the code works and produces good results. When running the CNN code, I came across an error that took me some time to fix; I was missing a line of code that flattens the output so that it could enter the linear layers. I will continue testing the CNN class tomorrow.  
**05/08/2024**  
I updated the CNN code to include training and test accuracy at the end of each epoch. These are helpful metrics to determine the performance and any possible overfitting. To prevent overfitting, I have added tunable dropout layers to my code; this will help stop the model from over-relying on nodes. I will experiment with this when testing the UJIIndoorLoc dataset. I have tested my CNN classifier class, and it showed poor performance; I think this is mainly due to the fact the synthetic data didn't paint a clear picture for it to discriminate against labels. I have now tested my CNN regressor on the synthetic data; the results were also poor. However, this may be due to the synthetic data not being separable enough, causing the high mean squared error.    
**06/08/2024**  
Now that I have finished testing the data on synthetic data, I'm applying the machine learning algorithms on the UJIIndoorLoc dataset. I have also made the document for the literature review on Wi-fi fingerprinting methods. I have been running the KNN Classifier on the competition dataset; the classifier tries to predict the building and floor for a given data sample.  
**08/08/2024**  
When running the KNN classifier on the competition dataset, I found it was slow, so I returned to my implementation and updated the code to improve the performance. I realised the code may be slow because the algorithm was storing all the neighbours, sorting the neighbours by distance, and extracting the number of neighbours. I have modified this function by storing the 'n' nearest neighbours only and ordering them in place as I add them to the nearest neighbour list. During my research, I found a better way to test the machine learning algorithms on synthetic data, so I will be revisiting this and reproducing the results on this newer method. I realised there was an error in my KNN class, which was stopping it from working correctly. The problem was that after finding a closer neighbour to the ones in the neighbour list, it replaced all neighbours further away from the target. I have now retested the KNN classifier on the various synthetic data I have produced. I have also applied my Decision Tree classifier to the synthetic data. I compared the results to the sklearn DTC implementation; they were similar to my DTC.  
**09/08/2024**  
When testing my SVM implementation, I found that changing the gamma hyperparameter did not change the result. I looked at my implementation and found that the hyperparameter was not passed correctly to the kernel function, which I have now amended. After fixing this issue, I began testing the SVM class on the synthetic data and compared the results to the sklearn SVC class. I have tested the CNN classifier on the synthetic data; I figured out two ways you can feed the data to the CNN model. You can use feature reduction methods to make it fit an NxN square or augment the data by adding zeros to the end of each sample to make it fit an NxN square. I used both methods on the synthetic data, and the results were promising. I have started making synthetic regression data using sklearn make_regression and sklearn make_blob (I used PCA to find the first two principal components and used them as the labels for make_blob). I have tested the Linear Regression model and KNN regressor on these synthetic datasets.  
**10/08/2024**  
Today I tested the remaining model on the synthetic data, I began by testing my Decision Tree Regressor class and realised that it needed to provide an r2 score. Therefore, I added this to my notebook so I could get a better understanding of the performance of the DTR. I then added the r2 score metric to my CNN regression implementation. At first, the CNN regression code wasn't working; however, I realised the shape of the prediction array was causing the problem. I then corrected this to make the model output correctly. I then tested the CNN regressor and found promising results. While working on the CNN class, I realised the KNN regressor class didn't have multi-target capabilities, so I added the necessary functionality and tested it on the synthetic data.  
